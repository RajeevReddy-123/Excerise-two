{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHOn-ZA1P0AR"
      },
      "source": [
        "## The second In-class-exercise (02/07/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXZbnBU9P0BP"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ozZbGVMP0BR"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1g91y4-XP0BS"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Reserach(Practical Question): The impact of social media usage and academic performance of college students\n",
        "Data that need to be collecetd: The number of hours per week spent on social media by college students along with their percentage levels.\n",
        "Data needed for the analysis: For a relevant analysis, at least 1000 data samples from college students would be required.\n",
        "Details steps for collecting and saving the data:\n",
        "1.Develop a survey questionnaire to gather the required information.\n",
        "2.Recruit college students to participate in the survey.\n",
        "3.Collect the data and store it in a spreadsheet or database.\n",
        "4.Clean and pre-process the data to ensure its quality and consistency.\n",
        "5.Save the cleaned data in a format suitable for analysis, such as a CSV file.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufo2kKwRP0Ba"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3H_K2oCP0Bb"
      },
      "outputs": [],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import csv # importing the csv library\n",
        "\n",
        "def collect_data(N):\n",
        "    data_sample = [] # takes the list of the smaple with any range\n",
        "    for i in range(N):\n",
        "        Std= {}\n",
        "        Std['Name'] = input(\"Enter the student's name: \") # details of the student\n",
        "        Std['Social_Media_Usage'] = float(input(\"Please indicate how many hours you spend each week on social media: \"))# timings spending on the socisl media by students\n",
        "        Std['GPA'] = float(input(\"Enter the student's GPA: \"))# Socila media effect on Students GPA\n",
        "        data_sample.append(Std)# here by using append function trying to get all the deatils into the std.\n",
        "    return data_sample # now by using the return the data which is append from the std into the data_sample.\n",
        "\n",
        "def save_data(data_sample): #creating a function for saving the data samples\n",
        "    with open('student_data.csv', 'w', newline='') as file: #By using the open we can get the csv file after the details are eneterd.\n",
        "        writer = csv.DictWriter(file, fieldnames=['Name', 'Social_Media_Usage', 'GPA'])# here by using the DictWriter teeling that the specifc filed names should be noted and avaiable in the dataset.\n",
        "        writer.writeheader()\n",
        "        for student in data_sample:\n",
        "            writer.writerow(student)# writerow helps to get the studnets details step by step\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    N = 1000 # 1000 data samples\n",
        "    data_sample = collect_data(N)\n",
        "    save_data(data_sample)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHbjGgyXP0Bc"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2012-2022).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests # requests helps to get the required url which consists of information retrieval\n",
        "import csv # csv is the file \n",
        "link = 'http://api.semanticscholar.org/graph/v1/paper/search?query=information+retrieval&offset=1000&limit=100&fields=title,authors,venue,year,abstract'# link thta provides the some information of the paper \n",
        "headers = {'Accept': 'application.json', \n",
        "           'Content Type' : 'application/json'}\n",
        "response = requests.request('GET', \n",
        "                            link, \n",
        "                            headers=headers, \n",
        "                            data = {})\n",
        "json_f = response.json()\n",
        "print(json_f)\n",
        "data = [] # collects the list of the various information\n",
        "columns = ['Title', 'Venue', 'Year', 'Authors', 'Abstract'] #specific columns that need to be presented in the paper\n",
        "for i in json_f['data']:\n",
        "    lists = [i['title'], i['venue'], i['year'], i['authors'], i['abstract']]\n",
        "    data.append(lists) # appending the lits values into the data\n",
        "with open('scrape.csv', 'w',\n",
        "          encoding = 'UTF8', newline = '') as f: # opening the csv file and converting the data into the UTF format\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(columns)\n",
        "    writer.writerows(data)\n",
        "print('File is in the destination.')# The destined file of the paper from the year between 2012-2022 year."
      ],
      "metadata": {
        "id": "2lbbC7T0WXfV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a07a80f3-c5cc-476d-f434-fc7c3db6dc93"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'total': 8591081, 'offset': 1000, 'next': 1100, 'data': [{'paperId': '5cf40fd7ca55729a162fbaf85a4a951d6a0a29db', 'title': 'A system for spoken query information retrieval on mobile devices', 'abstract': 'With the proliferation of handheld devices, information access on mobile devices is a topic of growing relevance. This paper presents a system that allows the user to search for information on mobile devices using spoken natural-language queries. We explore several issues related to the creation of this system, which combines state-of-the-art speech-recognition and information-retrieval technologies. This is the first work that we are aware of which evaluates spoken query based information retrieval on a commonly available and well researched text database, the Chinese news corpus used in the National Institute of Standards and Technology (NIST)s TREC-5 and TREC-6 benchmarks. To compare spoken-query retrieval performance for different relevant scenarios and recognition accuracies, the benchmark queries-read verbatim by 20 speakers-were recorded simultaneously through three channels: headset microphone, PDA microphone, and cellular phone. Our results show that for mobile devices with high-quality microphones, spoken-query retrieval based on existing technologies yields retrieval precisions that come close to that for perfect text input (mean average precision 0.459 and 0.489, respectively, on TREC-6).', 'venue': 'IEEE Transactions on Speech and Audio Processing', 'year': 2002, 'authors': [{'authorId': '1708592', 'name': 'Eric Chang'}, {'authorId': '1745715', 'name': 'F. Seide'}, {'authorId': '145199941', 'name': 'H. Meng'}, {'authorId': '2111499072', 'name': 'Zhuoran Chen'}, {'authorId': '145814852', 'name': 'Yu Shi'}, {'authorId': '1708065', 'name': 'Yuk-Chi Li'}]}, {'paperId': '349fd2000d792b53471cfd98decfd2cb5df5ac7d', 'title': 'Probabilistic models of ranking novel documents for faceted topic retrieval', 'abstract': 'Traditional models of information retrieval assume documents are independently relevant. But when the goal is retrieving diverse or novel information about a topic, retrieval models need to capture dependencies between documents. Such tasks require alternative evaluation and optimization methods that operate on different types of relevance judgments. We define faceted topic retrieval as a particular novelty-driven task with the goal of finding a set of documents that cover the different facets of an information need. A faceted topic retrieval system must be able to cover as many facets as possible with the smallest number of documents. We introduce two novel models for faceted topic retrieval, one based on pruning a set of retrieved documents and one based on retrieving sets of documents through direct optimization of evaluation measures. We compare the performance of our models to MMR and the probabilistic model due to Zhai et al. on a set of 60 topics annotated with facets, showing that our models are competitive.', 'venue': 'International Conference on Information and Knowledge Management', 'year': 2009, 'authors': [{'authorId': '1750995', 'name': 'Ben Carterette'}, {'authorId': '2667305', 'name': 'Praveen Chandar'}]}, {'paperId': '351e150e3242221b6bde689ea7ca3767e51f7cba', 'title': 'A temporal distinctiveness theory of recency and modality effects.', 'abstract': \"A temporal distinctiveness theory of contextually cued retrieval from memory is presented and applied to recency and modality effects. According to this theory, one part of the mnemonic trace of an item is a representation of the item's time of presentation. Time of presentation may be encoded with a coarse grain (so that it is consistent with a wide range of times) or with a fine grain (so that it is consistent with a narrow range of times). Retrieval proceeds by constructing temporally defined search sets that include representations of items consistent with the temporal bounds of the search set. The temporal width of the search set increases as the retention interval increases. Recency effects arise from retrieval of recently presented items from narrow search sets that include representations of few items; within the context of the search set, these items are distinctive and recalled well. Superiority in recall of recently presented auditory information in comparison with recently presented visual information is attributed to differences in the grain of time of presentation representations for aurally (fine grain) and visually (coarse grain) presented information. Four experiments confirm qualitative and quantitative predictions of the theory, including the prediction of auditory superiority at the beginning of the list when the initial items are temporally distinct.\", 'venue': 'Journal of Experimental Psychology. Learning, Memory and Cognition', 'year': 1986, 'authors': [{'authorId': '2258966', 'name': 'A. Glenberg'}, {'authorId': '143749966', 'name': 'N. Swanson'}]}, {'paperId': '44b711a04ee5d899a3ceef15ad33544c0c22fdaf', 'title': 'Transparent access to multiple bioinformatics information sources', 'abstract': 'This paper describes the Transparent Access to Multiple Bioinformatics Information Sources project, known as TAMBIS, in which a domain ontology for molecular biology and bioinformatics is used in a retrieval-based information integration system for biologists. The ontology, represented using a description logic and managed by a terminology server, is used both to drive a visual query interface and as a global schema against which complex intersource queries are expressed. These source-independent declarative queries are then rewritten into collections of ordered source-dependent queries for execution by a middleware layer. In bioinformatics, the majority of data sources are not databases but tools with limited accessible interfaces. The ontology helps manage the interoperation between these resources. The paper emphasizes the central role that is played by the ontology in the system. The project distinguishes itself from others in the following ways: the ontology, developed by a biologist, is substantial; the retrieval interface is sophisticated; the description logic is managed by a sophisticated terminology server. A full pilot application is available as a JavaTM applet integrating five sources concerned with proteins. This pilot is currently undergoing field trials with working biologists and is being used to answer real questions in biology, one of which is used as a case study throughout the paper.', 'venue': 'IBM Systems Journal', 'year': 2001, 'authors': [{'authorId': '46555127', 'name': 'C. Goble'}, {'authorId': '144560289', 'name': 'R. Stevens'}, {'authorId': '145624145', 'name': 'Gary Ng'}, {'authorId': '1709236', 'name': 'S. Bechhofer'}, {'authorId': '1716510', 'name': 'N. Paton'}, {'authorId': '2056288828', 'name': 'P. Baker'}, {'authorId': '2552247', 'name': 'M. Peim'}, {'authorId': '50325336', 'name': 'A. Brass'}]}, {'paperId': 'd6af7f6a638ee0cbe21e79e11b9e6f8fe352cb31', 'title': 'Evaluating user interfaces to information retrieval systems: a case study on user support', 'abstract': \"Designing good user interfaces to information retrieval systems is a complex activity. The design space is large and evaluation methodologies that go beyond the classical precision and recall figures are not well established. In this paper we present an evaluation of an intelligent interface that covers also the user-system interaction and measures user's satisfaction. More specifically, we describe an experiment that evaluates: (i) the added value of the semiautomatic query reformulation implemented in a prototype system; (ii) the importance of technical, terminological, and strategic supports and (iii) the best way to provide them. The interpretation of results leads to guidelines for the design of user interfaces to information retrieval systems and to some observations on the evaluation issue.\", 'venue': 'Annual International ACM SIGIR Conference on Research and Development in Information Retrieval', 'year': 1996, 'authors': [{'authorId': '1732821', 'name': 'G. Brajnik'}, {'authorId': '1726978', 'name': 'S. Mizzaro'}, {'authorId': '1705232', 'name': 'C. Tasso'}]}, {'paperId': '307f41d750862f5a25cdde7ea606c07c7b6f3889', 'title': 'A note on weighted queries in information retrieval systems', 'abstract': 'Methode de ponderation des termes de recherche dans le cadre des systemes relevant des ensembles flous', 'venue': 'Journal of the American Society for Information Science', 'year': 1987, 'authors': [{'authorId': '144127749', 'name': 'R. Yager'}]}, {'paperId': 'c6fd8cd780cd3fa99b14c25b14ee22b7c047e99c', 'title': 'Using character shape coding for information retrieval', 'abstract': \"In conventional information retrieval the task of finding users' search terms in a document is simple. When the document is not available in machine readable format, optical character recognition (OCR) can usually be performed. We have developed a technique for performing information retrieval on document images in such a manner that the accuracy has great utility. The method makes generalisations about the images of characters, then performs classification of these and agglomerates the resulting character shape codes into word tokens based on character shape coding. These are sufficiently specific in their representation of the underlying words to allow reasonable performance of retrieval. Using a collection of over 250 Mbytes of document texts and queries with known relevance assessments, we present a series of experiments to determine how various parameters in the retrieval strategy affect retrieval performance and we obtain a surprisingly good result.\", 'venue': 'Proceedings of the Fourth International Conference on Document Analysis and Recognition', 'year': 1997, 'authors': [{'authorId': '1680223', 'name': 'A. Smeaton'}, {'authorId': '145358278', 'name': 'A. Spitz'}]}, {'paperId': 'cd8fd3c8d280e1ba9b41de2e4822a54cd8e944c7', 'title': 'Language models for information retrieval', 'abstract': 'One of the major challenges in the field of information retrieval (IR) is to specify a formal framework that both describes the important processes involved in finding relevant information, and successfully predicts which techniques will provide good effectiveness in terms of accuracy. A recent approach that has shown considerable promise uses generative models of text (language models) to describe the IR processes. We briefly review the major variations of the language model approach and how they have been used to develop a range of retrieval-related language technologies, including cross-lingual IR and distributed search. We also discuss how this approach could be used with structured data extracted from text.', 'venue': 'Proceedings / International Conference on Data Engineering', 'year': 2003, 'authors': [{'authorId': '144456145', 'name': 'W. Bruce Croft'}]}, {'paperId': '44e98c35885e2fe5ae2937b4f01613ea2591bf1f', 'title': 'Searching distributed collections with inference networks', 'abstract': 'The use of information retrieval systems in networked environments raises a new set of issues that have received little attention. These issues include ranking document collections for relevance to a query, selecting the best set of collections from a ranked list, and merging the document rankings that are returned from a set of collections. This paper describes methods of addressing each issue in the inference network model, dkcusses their implementation in the INQUERY system, and presents experimental results demonstrating their effectiveness.', 'venue': 'Annual International ACM SIGIR Conference on Research and Development in Information Retrieval', 'year': 1995, 'authors': [{'authorId': '144987107', 'name': 'Jamie Callan'}, {'authorId': '2110326971', 'name': 'Zhihong Lu'}, {'authorId': '144456145', 'name': 'W. Bruce Croft'}]}, {'paperId': 'ba7ce0dd3d21771ae2093c9b722e8f55b6d8c1f3', 'title': 'Query Expansion with Locally-Trained Word Embeddings', 'abstract': 'Continuous space word embeddings have received a great deal of attention in the natural language processing and machine learning communities for their ability to model term similarity and other relationships. We study the use of term relatedness in the context of query expansion for ad hoc information retrieval. We demonstrate that word embeddings such as word2vec and GloVe, when trained globally, underperform corpus and query specific embeddings for retrieval tasks. These results suggest that other tasks benefiting from global embeddings may also benefit from local embeddings.', 'venue': 'Annual Meeting of the Association for Computational Linguistics', 'year': 2016, 'authors': [{'authorId': '145472333', 'name': 'Fernando Diaz'}, {'authorId': '116506812', 'name': 'Bhaskar Mitra'}, {'authorId': '1703980', 'name': 'Nick Craswell'}]}, {'paperId': 'ff7c02ac5cc7b02fa6cff6943bdbf2255a3ce935', 'title': 'Memory-Based Measures for Assessing Advertising Effects: A Comparison of Explicit and Implicit Memory Effects', 'abstract': 'Abstract Prior marketing studies investigating memory for advertisements have relied almost exclusively on examining effects contingent on explicit memory retrieval. This process involves a deliberate effort on the part of the consumer to think back to an advertisement in an attempt to recall the ad information. Studies in this area have shown that a lengthy delay between ad exposure and test, as well as divided attention during the ad exposure episode, hinder or even eliminate successful explicit memory retrieval. The premise of this paper is that an alternative retrieval process, implicit memory, may function differently. This form of memory retrieval is automatic in nature and does not rely on consumers deliberately searching their memory for a previously viewed advertisement. Comparisons with explicit memory retrieval suggest that implicit memory is preserved even in conditions of delay and divided attention, whereas explicit memory is affected detrimentally by those conditions. The two different forms of retrieval processes are validated with the use of a process dissociation procedure. Implications and directions for future research are discussed.', 'venue': '', 'year': 2001, 'authors': [{'authorId': '50161041', 'name': 'Stewart Shapiro'}, {'authorId': '40403103', 'name': 'H. S. Krishnan'}]}, {'paperId': '1953b8e7c6e22c70f9ec8f696e4810dc4755d507', 'title': 'From document retrieval to question answering', 'abstract': 'is one of the most valuable goods in modern society. With the rise of computers, storing huge amounts of data has become efficient and inexpensive. Although we are now in a position where we have unprecedented amounts of information at our finger tips, the question arises how to access these large amounts of data to find the information one is interested in. The issue of developing methods and tools for finding automatically relevant information is addressed by the research area of information retrieval, and, over the last decades, sophisticated document retrieval systems have been developed. One particular branch of information retrieval is question answering. Question answering systems enable users to pose full natural language questions, as opposed to keyword-based queries, which are commonly used in document retrieval. In recent years, question answering has witnessed a renaissance, which is mainly due to the availability of large corpora. Current question answering systems depend strongly on document retrieval as a means for identifying documents that are likely to contain answer to a given question. This thesis investigates the usefulness of different standard and novel document retrieval approaches in the context of question answering. More specifically, it compares them with respect to their ability to identify documents containing a correct answer. In addition, we also investigate to what extent the quality of a particular document retrieval approach has an impact on the overall performance of a specific question answering system.', 'venue': '', 'year': 2003, 'authors': [{'authorId': '1696402', 'name': 'Christof Monz'}]}, {'paperId': '34ddfe4d4fbf4afa586220a21a921478c8dfab35', 'title': 'Minimal test collections for retrieval evaluation', 'abstract': 'Accurate estimation of information retrieval evaluation metrics such as average precision require large sets of relevance judgments. Building sets large enough for evaluation of real-world implementations is at best inefficient, at worst infeasible. In this work we link evaluation with test collection construction to gain an understanding of the minimal judging effort that must be done to have high confidence in the outcome of an evaluation. A new way of looking at average precision leads to a natural algorithm for selecting documents to judge and allows us to estimate the degree of confidence by defining a distribution over possible document judgments. A study with annotators shows that this method can be used by a small group of researchers to rank a set of systems in under three hours with 95% confidence. Information retrieval metrics such as average precision require large sets of relevance judgments to be accurately estimated. Building these sets is infeasible and often inefficient for many real-world retrieval implementations. We present a new way of looking at average precision that allows us to estimate the confidence in an evaluation based on the size of the test collection. We use this to build an algorithm for selecting the best documents to judge to have maximum confidence in an evaluation with a minimal number of relevance judgments. A study with annotators shows how the algorithm can be used by a small group of researchers to quickly rank a set of systems with 95% confidence.', 'venue': 'Annual International ACM SIGIR Conference on Research and Development in Information Retrieval', 'year': 2006, 'authors': [{'authorId': '1750995', 'name': 'Ben Carterette'}, {'authorId': '144890574', 'name': 'James Allan'}, {'authorId': '143987348', 'name': 'R. Sitaraman'}]}, {'paperId': '8b444a80dfa41a65852483a974bb1192252aa554', 'title': 'Information retrieval based writer identification', 'abstract': 'This communication deals with the Writer Identificationtask. Our previous work has shown the interest of usingthe graphemes as features for describing the individualproperties of Handwriting. We propose here to exploit thesame feature set but using an information retrievalparadigm to describe and compare the handwritten queryto each sample of handwriting in the database. Using thistechnique the image processing stage is performed onlyonce and before the retrieval process can take place, thusleading to a significant saving in the computation of eachquery response, compared to our initial proposition. Themethod has been tested on two handwritten databases.The first one has been collected from 88 different writersat PSI Lab. while the second one contains 39 writers fromthe original correspondence of Emile Zola, a famousFrench novelist of the last 19th century. We also analyzethe proposed method when using concatenation ofgraphemes (bi and tri-gramme) as features.', 'venue': 'Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings.', 'year': 2003, 'authors': [{'authorId': '2390354', 'name': 'A. Bensefia'}, {'authorId': '1690399', 'name': 'T. Paquet'}, {'authorId': '1804638', 'name': 'L. Heutte'}]}, {'paperId': 'a3e5822306dfda985404d7f1fd3fd36e0fb6acb3', 'title': 'Inverse Modeling of Global and Regional CH4 Emissions Using SCIAMACHY Satellite Retrievals', 'abstract': 'Methane retrievals from the Scanning Imaging Absorption Spectrometer for Atmospheric Chartography (SCIAMACHY) instrument onboard ENVISAT provide important information on atmospheric CH_4 sources, particularly in tropical regions which are poorly monitored by in situ surface observations. Recently, Frankenberg et al. (2008a, 2008b) reported a major revision of SCIAMACHY retrievals due to an update of spectroscopic parameters of water vapor and CH_4. Here, we analyze the impact of this revision on global and regional CH_4 emissions estimates in 2004, using the TM5-4DVAR inverse modeling system. Inversions based on the revised SCIAMACHY retrievals yield ∼20% lower tropical emissions compared to the previous retrievals. The new retrievals improve significantly the consistency between observed and assimilated column average mixing ratios and the agreement with independent validation data. Furthermore, the considerable latitudinal and seasonal bias correction of the previous SCIAMACHY retrievals, derived in the TM5-4DVAR system by simultaneously assimilating high-accuracy surface measurements, is reduced by a factor of ∼3. The inversions result in significant changes in the spatial patterns of emissions and their seasonality compared to the bottom-up inventories. Sensitivity tests were done to analyze the robustness of retrieved emissions, revealing some dependence on the applied a priori emission inventories and OH fields. Furthermore, we performed a detailed validation of simulated CH_4 mixing ratios using NOAA ship and aircraft profile samples, as well as stratospheric balloon samples, showing overall good agreement. We use the new SCIAMACHY retrievals for a regional analysis of CH_4 emissions from South America, Africa, and Asia, exploiting the zooming capability of the TM5 model. This allows a more detailed analysis of spatial emission patterns and better comparison with aircraft profiles and independent regional emission estimates available for South America. Large CH_4 emissions are attributed to various wetland regions in tropical South America and Africa, seasonally varying and opposite in phase with CH_4 emissions from biomass burning. India, China and South East Asia are characterized by pronounced emissions from rice paddies peaking in the third quarter of the year, in addition to further anthropogenic emissions throughout the year.', 'venue': '', 'year': 2009, 'authors': [{'authorId': '1789254', 'name': 'P. Bergamaschi'}, {'authorId': '2579315', 'name': 'C. Frankenberg'}, {'authorId': '7880338', 'name': 'J. F. Meirink'}, {'authorId': '34892640', 'name': 'M. Krol'}, {'authorId': '48854163', 'name': 'M. Villani'}, {'authorId': '2633973', 'name': 'S. Houweling'}, {'authorId': '3993177', 'name': 'F. Dentener'}, {'authorId': '9385442', 'name': 'E. Dlugokencky'}, {'authorId': '152107839', 'name': 'John B. Miller'}, {'authorId': '39180211', 'name': 'L. Gatti'}, {'authorId': '2143909', 'name': 'A. Engel'}, {'authorId': '49482123', 'name': 'I. Levin'}]}, {'paperId': 'ed5ab0b05212355cd0637c3c50d13dc613bc1852', 'title': 'Exploiting clustering and phrases for context-based information retrieval', 'abstract': 'This paper explores exploiting the synergy between document clustering and phrasal analysis for the purpose of automatically constructing a corrrex~-busedretrieval system. A contex~ consists of two components a cluster of logically related articles (its exrension) and a small set of salient concepts, represented by words and phrases and organized by the cluster’s key terms (its irr~ertsion). At inn-time, the system presents contexts that best match the result list of a user’s natural language query. The user can then choose a context and manipulate the intensionsd component to both browse the context’s extension and launch new searches over the entire database. We argue that the focused relevance feedback provided by contexts, at a level of abstraction higher than individual documents and lower than the database as a whole, provides a natural way for users to refine vague information needs and helps to blur the distinction between searching and browsing. The I%zraphrase interface, running over a database of business-related news articles, is used to illustrate the advantages of such a context-based retrieval paradigm.', 'venue': 'Annual International ACM SIGIR Conference on Research and Development in Information Retrieval', 'year': 1997, 'authors': [{'authorId': '1723684', 'name': 'Peter G. Anick'}, {'authorId': '2066721', 'name': 'Shivakumar Vaithyanathan'}]}, {'paperId': '5aa9a530dee277bd40dbe8b081b8025439174ad1', 'title': 'Feature Location Using Probabilistic Ranking of Methods Based on Execution Scenarios and Information Retrieval', 'abstract': 'This paper recasts the problem of feature location in source code as a decision-making problem in the presence of uncertainty. The solution to the problem is formulated as a combination of the opinions of different experts. The experts in this work are two existing techniques for feature location: a scenario-based probabilistic ranking of events and an information-retrieval-based technique that uses latent semantic indexing. The combination of these two experts is empirically evaluated through several case studies, which use the source code of the Mozilla Web browser and the Eclipse integrated development environment. The results show that the combination of experts significantly improves the effectiveness of feature location as compared to each of the experts used independently', 'venue': 'IEEE Transactions on Software Engineering', 'year': 2007, 'authors': [{'authorId': '1697757', 'name': 'D. Poshyvanyk'}, {'authorId': '1718050', 'name': 'Yann-Gaël Guéhéneuc'}, {'authorId': '40473936', 'name': 'Andrian Marcus'}, {'authorId': '1692292', 'name': 'G. Antoniol'}, {'authorId': '3246103', 'name': 'V. Rajlich'}]}, {'paperId': '39f31b5f58f21d2acde9fc7b1a250a43377830a1', 'title': 'Toward a Cognitive Psychology of Syntax: Information Processing Contributions to Sentence Formulation', 'abstract': 'it is widely acknowledged that characteristics of the general information processing svstem in which sentence formulation occurs mav provide constraints on syntax in language use. This paper proposes one possibli&urce of such constraints. Evidence is reviewed indicating that the syntax of sentences may .to some degree reflect the transient processing demands of lexical retrieval, suggesting an interaction between syntactic and lexical processing. Specifically, the syntactic structure of utterances appears to be sensitive to the accessibility of lexical information, with phrases containing more accessible information occurring earlier in sentences. The existence of such an interaction argues that the utterance formulation system is not strictly hierarchical, as most current approaches to sentence production imply. A broad framework for models of production is outlined that incorporates these interactions within a limited-capacity processing system. This framework also permits a resolution of contradictions in the literature on pragmatic determinants of constituent order in adult language use.', 'venue': '', 'year': 1982, 'authors': [{'authorId': '47253252', 'name': 'K. Bock'}]}, {'paperId': '3c1e2f63709f2b2371b91c62d3d819b9d4a7f0b2', 'title': 'Spatial-bag-of-features', 'abstract': \"In this paper, we study the problem of large scale image retrieval by developing a new class of bag-of-features to encode geometric information of objects within an image. Beyond existing orderless bag-of-features, local features of an image are first projected to different directions or points to generate a series of ordered bag-of-features, based on which different families of spatial bag-of-features are designed to capture the invariance of object translation, rotation, and scaling. Then the most representative features are selected based on a boosting-like method to generate a new bag-of-features-like vector representation of an image. The proposed retrieval framework works well in image retrieval task owing to the following three properties: 1) the encoding of geometric information of objects for capturing objects' spatial transformation, 2) the supervised feature selection and combination strategy for enhancing the discriminative power, and 3) the representation of bag-of-features for effective image matching and indexing for large scale image retrieval. Extensive experiments on 5000 Oxford building images and 1 million Panoramio images show the effectiveness and efficiency of the proposed features as well as the retrieval framework.\", 'venue': '2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition', 'year': 2010, 'authors': [{'authorId': '145871531', 'name': 'Yang Cao'}, {'authorId': '1906061249', 'name': 'Changhu Wang'}, {'authorId': '2109689495', 'name': 'Zhiwei Li'}, {'authorId': '2108911758', 'name': 'Liqing Zhang'}, {'authorId': '39089563', 'name': 'Lei Zhang'}]}, {'paperId': '94297fc4c2b1e663576dca65fe2f60be74f8066e', 'title': 'A theory of information need for information retrieval that connects information to knowledge', 'abstract': \"This article proposes a theory of information need for information retrieval (IR). Information need traditionally denotes the start state for someone seeking information, which includes information search using an IR system. There are two perspectives on information need. The dominant, computer science perspective is that the user needs to find an answer to a well-defined question which is easy for the user to formulate into a query to the system. Ironically, information science's best known model of information need (Taylor, 1968) deems it to be a “black box”—unknowable and nonspecifiable by the user in a query to the information system. Information science has instead devoted itself to studying eight adjacent or surrogate concepts (information seeking, search and use; problem, problematic situation and task; sense making and evolutionary adaptation/information foraging). Based on an analysis of these eight adjacent/surrogate concepts, we create six testable propositions for a theory of information need. The central assumption of the theory is that while computer science sees IR as an information- or answer-finding system, focused on the user finding an answer, an information science or user-oriented theory of information need envisages a knowledge formulation/acquisition system. © 2011 Wiley Periodicals, Inc.\", 'venue': 'J. Assoc. Inf. Sci. Technol.', 'year': 2011, 'authors': [{'authorId': '144614321', 'name': 'C. Cole'}]}, {'paperId': '51cb8826eafa4c22a0906e50905132186a6fa225', 'title': 'Modeling Vagueness in Information Retrieval', 'abstract': None, 'venue': 'European Summer School in Information Retrieval', 'year': 2000, 'authors': [{'authorId': '2637401', 'name': 'Gloria Bordogna'}, {'authorId': '145645971', 'name': 'G. Pasi'}]}, {'paperId': 'de90778bf9548820f32d5951ea936004ff17baff', 'title': 'Image retrieval with geometry-preserving visual phrases', 'abstract': 'The most popular approach to large scale image retrieval is based on the bag-of-visual-word (BoV) representation of images. The spatial information is usually re-introduced as a post-processing step to re-rank the retrieved images, through a spatial verification like RANSAC. Since the spatial verification techniques are computationally expensive, they can be applied only to the top images in the initial ranking. In this paper, we propose an approach that can encode more spatial information into BoV representation and that is efficient enough to be applied to large-scale databases. Other works pursuing the same purpose have proposed exploring the word co-occurrences in the neighborhood areas. Our approach encodes more spatial information through the geometry-preserving visual phrases (GVP). In addition to co-occurrences, the GVP method also captures the local and long-range spatial layouts of the words. Our GVP based searching algorithm increases little memory usage or computational time compared to the BoV method. Moreover, we show that our approach can also be integrated to the min-hash method to improve its retrieval accuracy. The experiment results on Oxford 5K and Flicker 1M dataset show that our approach outperforms the BoV method even following a RANSAC verification.', 'venue': 'Computer Vision and Pattern Recognition', 'year': 2011, 'authors': [{'authorId': '2145113691', 'name': 'Yimeng Zhang'}, {'authorId': '3243563', 'name': 'Zhaoyin Jia'}, {'authorId': '40894914', 'name': 'Tsuhan Chen'}]}, {'paperId': '9ffa5bb2995d90931454bc78b894a868f5c22799', 'title': 'Evaluation of a tool for visualization of information retrieval results', 'abstract': 'We report on the design and evaluation of a visualization tool for Information Retrieval (IR) systems that aims to help the end user in the following respects: • As an indicator of document relevance, the tool graphically provides specific query related information about individual documents • As a diagnosis tool, it graphically provides aggregate information about the query results that could help in identifying how the different query terms influence the retrieval and ranking of documents. Two different experiments using TREC-4 data were conducted to evaluate the effectiveness of this tool. Results, while mixed, indicate that visualization of this sort may provide useful support for judging the relevance of documents, in particular by enabling users to make more accurate decisions about which documents to inspect in detail. Problems in evaluation of such tools in interactive environments are discussed.', 'venue': 'Annual International ACM SIGIR Conference on Research and Development in Information Retrieval', 'year': 1996, 'authors': [{'authorId': '2566112', 'name': 'A. Veerasamy'}, {'authorId': '2095083', 'name': 'N. Belkin'}]}, {'paperId': '327edcc76d35e0e1014b4d9d73d1e16cc42697a5', 'title': 'Analysis and evaluation of visual information systems performance', 'abstract': 'This dissertation investigates the system-centred evaluation of visual information retrieval from generic photographic collections. The development of visual information retrieval systems has long been hindered by the lack of standardised benchmarks. Researchers have proposed numerous systems and techniques, and although different systems clearly have their particular strength, there is a tendency by researchers to use different means of showing retrieval performance to highlight the own algorithm’s benefits. For the field of visual information search to advance, however, objective evaluation to identify, compare and validate the strengths and merits of different systems is therefore essential. Benchmarks to carry out such evaluation have recently been developed, and evaluation events have also been organised for several domains. Yet, no efforts have considered the evaluation of retrieval from generic photographic collections (i.e. containing everyday real-world photographs akin to those that can frequently be found in private photographic collections as well, e.g. pictures of holidays and events). We therefore first analyse a multitude of variables and factors with respect to the performance and requirements of visual information systems, and we then design and implement the framework and resources necessary to carry out such an evaluation. These resources include: a parametric image collection, representative search requests, relevance assessments and a set of performance measures. In addition, we organise the first evaluation event for retrieval from generic photographic collections and report on its realisation. Finally, we present an analysis and the evaluation of the participating retrieval systems as well as of the evaluation event itself. Filling this particular gap by making possible a systematic calibration and comparison of system performance for retrieval from generic photographic collections constitutes the main scientific contribution of this research. This dissertation thereby enables a deeper understanding of the complex conditions and constraints associated with visual information identification, the accurate capturing of user requirements, the appropriate specification and complexity of user queries, the execution of searches, and the reliability of performance indicators.', 'venue': '', 'year': 2007, 'authors': [{'authorId': '2426894', 'name': 'Michael Grubinger'}]}, {'paperId': 'f4a1cbae88edd809bc6e6d4b66034a65eae62083', 'title': 'Braque: Design of an Interface to Support User Interaction in Information Retrieval', 'abstract': None, 'venue': 'Information Processing & Management', 'year': 1993, 'authors': [{'authorId': '2095083', 'name': 'N. Belkin'}, {'authorId': '32111253', 'name': 'P. G. Marchetti'}, {'authorId': '145292542', 'name': 'Colleen Cool'}]}, {'paperId': 'c5c6ea2f23fe8d3e986c4c99e83a90c204538619', 'title': 'Regularization and Semi-supervised Learning on Large Graphs', 'abstract': None, 'venue': 'Annual Conference Computational Learning Theory', 'year': 2004, 'authors': [{'authorId': '145520115', 'name': 'Mikhail Belkin'}, {'authorId': '2458509', 'name': 'Irina Matveeva'}, {'authorId': '1770745', 'name': 'P. Niyogi'}]}, {'paperId': '1ce659da72de0a9eb0f9a636d02f76803765575c', 'title': 'A new approach for LSB based image steganography using secret key', 'abstract': 'This paper introduces a best approach for Least Significant Bit (LSB) based on image steganography that enhances the existing LSB substitution techniques to improve the security level of hidden information. It is a new approach to substitute LSB of RGB true color image. The new security conception hides secret information within the LSB of image where a secret key encrypts the hidden information to protect it from unauthorized users. In general, in LSB methods, hidden information is stored into a specific position of LSB of image. For this reason, knowing the retrieval methods, anyone can extract the hidden information. In our paper, hidden information is stored into different position of LSB of image depending on the secret key. As a result, it is difficult to extract the hidden information knowing the retrieval methods. We have used the Peak Signal-to-Noise Ratio (PSNR) to measure the quality of the stego images. The value of PSNR gives better result because our proposed method changes very small number of bits of the image. The obtained results show that the proposed method results in LSB based image steganography using secret key which provides good security issue and PSNR value than general LSB based image steganography methods.', 'venue': '14th International Conference on Computer and Information Technology (ICCIT 2011)', 'year': 2011, 'authors': [{'authorId': '9290361', 'name': 'S. M. Masud Karim'}, {'authorId': '145968760', 'name': 'M. S. Rahman'}, {'authorId': '2110866490', 'name': 'Md. Ismail Hossain'}]}, {'paperId': '875855d9c6a2b6eca44578e2937f3de7c2cec31d', 'title': 'Retrieval and unification of syntactic structure in sentence comprehension: an FMRI study using word-category ambiguity.', 'abstract': 'Sentence comprehension requires the retrieval of single word information from long-term memory, and the integration of this information into multiword representations. The current functional magnetic resonance imaging study explored the hypothesis that the left posterior temporal gyrus supports the retrieval of lexical-syntactic information, whereas left inferior frontal gyrus (LIFG) contributes to syntactic unification. Twenty-eight subjects read sentences and word sequences containing word-category (noun-verb) ambiguous words at critical positions. Regions contributing to the syntactic unification process should show enhanced activation for sentences compared to words, and only within sentences display a larger signal for ambiguous than unambiguous conditions. The posterior LIFG showed exactly this predicted pattern, confirming our hypothesis that LIFG contributes to syntactic unification. The left posterior middle temporal gyrus was activated more for ambiguous than unambiguous conditions (main effect over both sentences and word sequences), as predicted for regions subserving the retrieval of lexical-syntactic information from memory. We conclude that understanding language involves the dynamic interplay between left inferior frontal and left posterior temporal regions.', 'venue': 'Cerebral Cortex', 'year': 2009, 'authors': [{'authorId': '40150546', 'name': 'T. M. Snijders'}, {'authorId': '1893662', 'name': 'T. Vosse'}, {'authorId': '144733915', 'name': 'G. Kempen'}, {'authorId': '73776184', 'name': 'J. V. van Berkum'}, {'authorId': '3071782', 'name': 'K. Petersson'}, {'authorId': '2608476', 'name': 'P. Hagoort'}]}, {'paperId': 'f434a43cf567a68154fad3f324fad399cfcc8b13', 'title': 'Finding Information on the World Wide Web: The Retrieval Effectiveness of Search Engines', 'abstract': None, 'venue': 'Information Processing & Management', 'year': 1999, 'authors': [{'authorId': '20094419', 'name': 'Michael D. Gordon'}, {'authorId': '144340728', 'name': 'Praveen Pathak'}]}, {'paperId': 'e097a6af10dbd8780f300c6758f8d28154292272', 'title': 'Less is More: Efficient 3-D Object Retrieval With Query View Selection', 'abstract': 'The explosively increasing 3-D objects make their efficient retrieval technology highly desired. Extensive research efforts have been dedicated to view-based 3-D object retrieval for its advantage of using 2-D views to represent 3-D objects. In this paradigm, typically the retrieval is accomplished by matching the views of the query object with the objects in database. However, using all the query views may not only introduce difficulty in rapid retrieval but also degrade retrieval accuracy when there is a mismatch between the query views and the object views in the database. In this work, we propose an interactive 3-D object retrieval scheme. Given a set of query views, we first perform clustering to obtain several candidates. We then incrementally select query views for object matching: in each round of relevance feedback, we only add the query view that is judged to be the most informative one based on the labeling information. In addition, we also propose an efficient approach to learn a distance metric for the newly selected query view and the weights for combining all of the selected query views. We conduct experiments on the National Taiwan University 3D Model database, ETH 3D object collection, and Shape Retrieval Content of Non-Rigid 3D Model, and results demonstrated that our approach not only significantly speeds up the retrieval process but also achieves encouraging retrieval performance.', 'venue': 'IEEE transactions on multimedia', 'year': 2011, 'authors': [{'authorId': '35350470', 'name': 'Yue Gao'}, {'authorId': '47446553', 'name': 'Meng Wang'}, {'authorId': '143962510', 'name': 'Zhengjun Zha'}, {'authorId': '144876831', 'name': 'Q. Tian'}, {'authorId': '144954808', 'name': 'Qionghai Dai'}, {'authorId': '2178247', 'name': 'Naiyao Zhang'}]}, {'paperId': '58c338f9f912b61e3fa65ab504c36a41508a68b1', 'title': 'Explicit Versus Latent Concept Models for Cross-Language Information Retrieval', 'abstract': 'The field of information retrieval and text manipulation (classification, clustering) still strives for models allowing semantic information to be folded in to improve performance with respect to standard bag-of-word based models. Many approaches aim at a concept-based retrieval, but differ in the nature of the concepts, which range from linguistic concepts as defined in lexical resources such as WordNet, latent topics derived from the data itself - as in Latent Semantic Indexing (LSI) or (Latent Dirichlet Allocation (LDA) - to Wikipedia articles as proxies for concepts, as in the recently proposed Explicit Semantic Analysis (ESA) model. A crucial question which has not been answered so far is whether models based on explicitly given concepts (as in the ESA model for instance) perform inherently better than retrieval models based on \"latent\" concepts (as in LSI and/or LDA). In this paper we investigate this question closer in the context of a cross-language setting, which inherently requires concept-based retrieval bridging between different languages. In particular, we compare the recently proposed ESA model with two latent models (LSI and LDA) showing that the former is clearly superior to the both. From a general perspective, our results contribute to clarifying the role of explicit vs. implicitly derived or latent concepts in (cross-language) information retrieval research.', 'venue': 'International Joint Conference on Artificial Intelligence', 'year': 2009, 'authors': [{'authorId': '1748977', 'name': 'P. Cimiano'}, {'authorId': '46312930', 'name': 'A. Schultz'}, {'authorId': '144589889', 'name': 'Sergej Sizov'}, {'authorId': '2871480', 'name': 'Philipp Sorg'}, {'authorId': '1752093', 'name': 'Steffen Staab'}]}, {'paperId': '91f92ca8b4fe294c9a1eb4f5c6f2a2b84030b8d1', 'title': 'A well‐calibrated ocean algorithm for special sensor microwave / imager', 'abstract': 'I describe an algorithm for retrieving geophysical parameters over the ocean from special sensor microwave/imager (SSM/I) observations. This algorithm is based on a model for the brightness temperature T(sub B) of the ocean and intervening atmosphere. The retrieved parameters are the near-surface wind speed W, the columnar water vapor V, the columnar cloud liquid water L, and the line-of-sight wind W(sub LS). I restrict my analysis to ocean scenes free of rain, and when the algorithm detects rain, the retrievals are discarded. The model and algorithm are precisely calibrated using a very large in situ database containing 37,650 SSM/I overpasses of buoys and 35,108 overpasses of radiosonde sites. A detailed error analysis indicates that the T(sub B) model rms accuracy is between 0.5 and 1 K and that the rms retrieval accuracies for wind, vapor, and cloud are 0.9 m/s, 1.2 mm, and 0.025 mm, respectively. The error in specifying the cloud temperature will introduce an additional 10% error in the cloud water retrieval. The spatial resolution for these accuracies is 50 km. The systematic errors in the retrievals are smaller than the rms errors, being about 0.3 m/s, 0.6 mm, and 0.005 mm for W, V, and L, respectively. The one exception is the systematic error in wind speed of -1.0 m/s that occurs for observations within +/-20 deg of upwind. The inclusion of the line-of-sight wind W(sub LS) in the retrieval significantly reduces the error in wind speed due to wind direction variations. The wind error for upwind observations is reduced from -3.0 to -1.0 m/s. Finally, I find a small signal in the 19-GHz, horizontal polarization (h(sub pol) T(sub B) residual DeltaT(sub BH) that is related to the effective air pressure of the water vapor profile. This information may be of some use in specifying the vertical distribution of water vapor.', 'venue': '', 'year': 1997, 'authors': [{'authorId': '32534296', 'name': 'F. Wentz'}]}, {'paperId': '812b1acda9416e750b40bb721673174004852048', 'title': 'Information Retrieval in MML', 'abstract': None, 'venue': 'Mathematical Knowledge Management', 'year': 2003, 'authors': [{'authorId': '1697055', 'name': 'G. Bancerek'}, {'authorId': '145315690', 'name': 'P. Rudnicki'}]}, {'paperId': 'f2fc074cfcdac137ef415929e956c3cfca49fe5b', 'title': 'Online Information Retrieval: Concepts, Principles and Techniques', 'abstract': 'Here is a clear, concise analysis of the concepts, principles, and techniques of online information searching and retrieval. This volume provides its readers with the basic framework necessary for understanding and critically evaluating an online search. The author has bridged the gap between theory and application in his coverage of the funamentals of this problem-solving process.', 'venue': '', 'year': 1986, 'authors': [{'authorId': '34986152', 'name': 'S. P. Harter'}]}, {'paperId': 'bb186a6b5cc0047450d392a6f711545d06a065dc', 'title': 'Context-Aware Retrieval for Ubiquitous Computing Environments', 'abstract': None, 'venue': 'Mobile HCI Workshop on Mobile and Ubiquitous Information Access', 'year': 2003, 'authors': [{'authorId': '143723939', 'name': 'G. Jones'}, {'authorId': '2115084031', 'name': 'P. Brown'}]}, {'paperId': 'e1cbe870e1f375639eb5e3afaf968e34ebea4286', 'title': 'Optimization of inverted vector searches', 'abstract': 'A simple algorithm is presented for increasing the efficiency of information retrieval searches which are implemented using inverted files. This optimization algorithm employs knowledge about the methods used for weighting document and query terms in order to examine as few inverted lists as possible. An extension to the basic algorithm allows greatly increased performance optimization at a modest cost in retrieval effectiveness. Experimental runs are made examining several different term weighting models and showing the optimization possible with each.', 'venue': 'Annual International ACM SIGIR Conference on Research and Development in Information Retrieval', 'year': 1985, 'authors': [{'authorId': '144009691', 'name': 'C. Buckley'}, {'authorId': '2439445', 'name': 'A. F. Lewit'}]}, {'paperId': 'e98ab6eb7fad7b9e8c4473fb92e4fe61cc450659', 'title': 'Survey on mining subjective data on the web', 'abstract': None, 'venue': 'Data mining and knowledge discovery', 'year': 2012, 'authors': [{'authorId': '2245199', 'name': 'Mikalai Tsytsarau'}, {'authorId': '1725167', 'name': 'Themis Palpanas'}]}, {'paperId': 'cc8973f0355bb953a043ad7d535a73e3c297954a', 'title': 'Predicted errors of tropospheric emission spectrometer nadir retrievals from spectral window selection', 'abstract': '[1]\\xa0Error covariances and vertical resolutions are reported for Tropospheric Emission Spectrometer (TES) nadir-view retrievals of surface temperature, atmospheric temperature, H2O, O3, CO, and CH4. These error covariances are computed as a result of selecting spectral windows that maximize the information content of simulated, TES nadir-view atmospheric retrievals of four regions representative of northern midlatitude, southern midlatitude, tropical, and polar climates. The information content of a retrieval is a function of an a priori and an a posteriori covariance matrix where the a posteriori covariance depends on an estimated smoothing error, measurement error, and systematic errors from interfering species, surface emissivity, atmospheric and surface temperature, and line parameter uncertainties. For conditions representative of northern midlatitudes, we can expect about 3 degrees of freedom (DOF) for retrievals of H2O, 5 DOF for O3 with about 2.4 DOF in the troposphere, and 0.8 DOF for CO. These measures for the vertical resolution and the predicted errors can be used to assess which atmospheric science questions can be addressed with TES atmospheric retrievals. Proper characterization of TES retrievals is also critical for applications such as atmospheric data assimilation and inverse modeling.', 'venue': '', 'year': 2004, 'authors': [{'authorId': '5280964', 'name': 'J. Worden'}, {'authorId': '2842010', 'name': 'S. Kulawik'}, {'authorId': '28292974', 'name': 'M. Shephard'}, {'authorId': '3278870', 'name': 'S. Clough'}, {'authorId': '2173967', 'name': 'H. Worden'}, {'authorId': '2172507', 'name': 'K. Bowman'}, {'authorId': '145852244', 'name': 'A. Goldman'}]}, {'paperId': '6b3853f08c482fe1bfbe39d656d50a8c73976f3c', 'title': 'Development of a stemming algorithm', 'abstract': 'A stemming algorithm, a procedure to reduce all words with the same stem to a common form, is useful in many areas of computational linguistics and information-retrieval work. While the form of the algorithm varies with its application, certain linguistic problems are common to any stemming procedure. As a basis for evaluation of previous attempts to deal with these problems, this paper first discusses the theoretical and practical attributes of stemming algorithms. Then a new version of a context-sensitive, longest-match stemming algorithm for English is proposed; though developed for use in a library information transfer system, it is of general application. A major linguistic problem in stemming, variation in spelling of stems, is discussed in some detail and several feasible programmed solutions are outlined, along with sample results of one of these methods.', 'venue': 'Mechanical Translation and Computational Linguistics', 'year': 1968, 'authors': [{'authorId': '66109618', 'name': 'J. B. Lovins'}]}, {'paperId': '90fa87b5b153ff90f6e4a5b21ca267842c172863', 'title': 'Advances in Information Retrieval', 'abstract': None, 'venue': 'Lecture Notes in Computer Science', 'year': 2015, 'authors': [{'authorId': '1699657', 'name': 'A. Hanbury'}, {'authorId': '1688470', 'name': 'G. Kazai'}, {'authorId': '145494588', 'name': 'A. Rauber'}, {'authorId': '1703148', 'name': 'N. Fuhr'}]}, {'paperId': '88eac46b7ab6b46682330926a94c30b999c74b9e', 'title': 'Multilingual Information Retrieval: From Research To Practice', 'abstract': 'Introduction.- Within-Language Information Retrieval.- Cross-Language Information Retrieval.- Interaction and User Interfaces.- Evaluation for Multilingual Information Retrieval Systems.- Applications of Multilingual Information Access.', 'venue': '', 'year': 2012, 'authors': [{'authorId': '144423157', 'name': 'C. Peters'}, {'authorId': '3075644', 'name': 'Martin Braschler'}, {'authorId': '1704149', 'name': 'Paul D. Clough'}]}, {'paperId': '471cb4c2e5039bdaacb0274fee70c7fe2e93493e', 'title': 'Rank-biased precision for measurement of retrieval effectiveness', 'abstract': 'A range of methods for measuring the effectiveness of information retrieval systems has been proposed. These are typically intended to provide a quantitative single-value summary of a document ranking relative to a query. However, many of these measures have failings. For example, recall is not well founded as a measure of satisfaction, since the user of an actual system cannot judge recall. Average precision is derived from recall, and suffers from the same problem. In addition, average precision lacks key stability properties that are needed for robust experiments. In this article, we introduce a new effectiveness metric, rank-biased precision, that avoids these problems. Rank-biased pre-cision is derived from a simple model of user behavior, is robust if answer rankings are extended to greater depths, and allows accurate quantification of experimental uncertainty, even when only partial relevance judgments are available.', 'venue': 'TOIS', 'year': 2008, 'authors': [{'authorId': '144448479', 'name': 'Alistair Moffat'}, {'authorId': '1751206', 'name': 'J. Zobel'}]}, {'paperId': 'aa738edf39e0b3a57b2d7c250a82c017e9796826', 'title': 'Stress and glucocorticoids impair retrieval of long-term spatial memory', 'abstract': None, 'venue': 'Nature', 'year': 1998, 'authors': [{'authorId': '121913584', 'name': 'D. D. de Quervain'}, {'authorId': '2890013', 'name': 'B. Roozendaal'}, {'authorId': '3008144', 'name': 'J. D. McGaugh'}]}, {'paperId': '125250e97714592b8437031de631bcda469d02a6', 'title': 'VideoQ: an automated content based video search system using visual cues', 'abstract': 'The rapidity with which digitat information, particularly video, is being generated, has necessitated the development of tools for efficient search of these media. Content based visual queries have been primarily focussed on still image retrieval. In this papel; we propose a novel, real-time, interactive system on the Web, based on the visual paradigm, with spatio-temporal attributesplaying a key role in video retrieval. We have developed algorithms for automated video object segmentation and tracking and use real-time video editing techniques while responding to user queries. The resulting system pe$orms well, with the user being able to retrieve complex video clips such as those of skiers, baseball players, with ease.', 'venue': \"MULTIMEDIA '97\", 'year': 1997, 'authors': [{'authorId': '9546964', 'name': 'Shih-Fu Chang'}, {'authorId': '2109082247', 'name': 'William Chen'}, {'authorId': '2069339', 'name': 'H. Meng'}, {'authorId': '145372008', 'name': 'H. Sundaram'}, {'authorId': '40588714', 'name': 'D. Zhong'}]}, {'paperId': '06ea464e929abfa514f970397df8e9943d7b2f7b', 'title': 'GeoCLEF 2008: the CLEF 2008 Cross-Language Geographic Information Retrieval Track Overview', 'abstract': None, 'venue': 'Conference and Labs of the Evaluation Forum', 'year': 2008, 'authors': [{'authorId': '32416012', 'name': 'Thomas Mandl'}, {'authorId': '1772996', 'name': 'F. Gey'}, {'authorId': '1695690', 'name': 'Giorgio Maria Di Nunzio'}, {'authorId': '143612982', 'name': 'N. Ferro'}, {'authorId': '1803434', 'name': 'R. Larson'}, {'authorId': '144721996', 'name': 'M. Sanderson'}, {'authorId': '145928342', 'name': 'Diana Santos'}, {'authorId': '1400935637', 'name': 'Christa Womser-Hacker'}, {'authorId': '2110972758', 'name': 'Xing Xie'}]}, {'paperId': 'ceb03a94ee33926a1959dc239854eb4b941e7af2', 'title': 'On the Use of Information Retrieval Measures for Speech Recognition Evaluation', 'abstract': 'This paper discusses the evaluation of automatic speech recognition (ASR) systems developed for practical applications, suggesting a set of criteria for application-oriented performance measures. The commonly used word error rate (WER), which poses ASR evaluation as a string editing process, is shown to have a number of limitations with respect to these criteria, motivating alternative or additional measures. This paper suggests that posing speech recognition evaluation as an information retrieval problem, where each word is one unit of information, offers a flexible framework for application-oriented performance analysis based on the concepts of recall and precision.', 'venue': '', 'year': 2004, 'authors': [{'authorId': '1762976', 'name': 'I. McCowan'}, {'authorId': '143929295', 'name': 'Darren Moore'}, {'authorId': '37787729', 'name': 'J. Dines'}, {'authorId': '1403029865', 'name': 'D. Gática-Pérez'}, {'authorId': '38059118', 'name': 'Mike Flynn'}, {'authorId': '2751729', 'name': 'P. Wellner'}, {'authorId': '1733733', 'name': 'H. Bourlard'}]}, {'paperId': '7526fd57b4ea124b2081a222fdc9fbbd628a987b', 'title': 'Spatial information retrieval and geographical ontologies an overview of the SPIRIT project', 'abstract': \"A large proportion of the resources available on the world-wide \\nweb refer to information that may be regarded as geographically \\nlocated. Thus most activities and enterprises take place in one or \\nmore places on the Earth's surface and there is a wealth of survey \\ndata, images, maps and reports that relate to specific places or \\nregions. Despite the prevalence of geographical context, existing \\nweb search facilities are poorly adapted to help people find \\ninformation that relates to a particular location. When the name of \\na place is typed into a typical search engine, web pages that \\ninclude that name in their text will be retrieved, but it is likely \\nthat many resources that are also associated with the place may \\nnot be retrieved. Thus resources relating to places that are inside \\nthe specified place may not be found, nor may be places that are \\nnearby or that are equivalent but referred to by another name. \\nSpecification of geographical context frequently requires the use \\nof spatial relationships concerning distance or containment for \\nexample, yet such terminology cannot be understood by existing \\nsearch engines. Here we provide a brief survey of existing \\nfacilities for geographical information retrieval on the web, before \\ndescribing a set of tools and techniques that are being developed \\nin the project SPIRIT : Spatially-Aware Information Retrieval on \\nthe Internet (funded by European Commission Framework V \\nProject IST-2001-35047).\", 'venue': 'Annual International ACM SIGIR Conference on Research and Development in Information Retrieval', 'year': 2002, 'authors': [{'authorId': '145939538', 'name': 'Christopher B. Jones'}, {'authorId': '48713956', 'name': 'R. Purves'}, {'authorId': '144273937', 'name': 'A. Ruas'}, {'authorId': '144721996', 'name': 'M. Sanderson'}, {'authorId': '34946374', 'name': 'Monika Sester'}, {'authorId': '1796774', 'name': 'M. V. Kreveld'}, {'authorId': '145525783', 'name': 'R. Weibel'}]}, {'paperId': '327efd65ba18aef17379fcc0c7dc7162e06339a6', 'title': 'SIR: A COMPUTER PROGRAM FOR SEMANTIC INFORMATION RETRIEVAL', 'abstract': 'SIR is a computer system, programmed in the LISP language, which accepts information and answers questions expressed in a restricted form of English. This system demonstrates what can reasonably be called an ability to \"understand\" semantic information. SIR\\'\\'s semantic and deductive ability is based on the construction of an internal model, which uses word associations and property lists, for the relational information normally conveyed in conversational statements. A format-matching procedure extracts semantic content from English sentences. If an input sentence is declarative, the system adds appropriate information to the model. If an input sentence is a question, the system searches the model until it either finds the answer or determines why it cannot find the answer. In all cases SIR reports its conclusions. The system has some capacity to recognize exceptions to general rules, resolve certain semantic ambiguities, and modify its model structure in order to save computer memory space. Judging from its conversational ability, SIR is more \"intelligent\" than any existing question-answering system. The author describes how this ability was developed and how the basic features of SIR compare with those of other systems. The working system, SIR , is a first step toward intelligent machine communication. The author proposes a next step by describing how to construct a more general system which is less complex and yet more powerful than SIR . This proposed system contains a generalized version of the SIR model, a formal logical system called SIR1 , and a computer program for testing the truth of SIR1 statements with respect to the generalized model by using partial proof procedures in the predicate calculus. The thesis also describes the formal properties of SIR1 and how they relate to the logical structure of SIR .', 'venue': '', 'year': 1964, 'authors': [{'authorId': '28791020', 'name': 'B. Raphael'}]}, {'paperId': '98487643d3477d73f5d739b42d59ae5b9ef06ee4', 'title': 'The Role of Parietal Cortex in Verbal Working Memory', 'abstract': 'Neuroimaging studies of normal subjects and studies of patients with focal lesions implicate regions of parietal cortex in verbal working memory (VWM), yet the precise role of parietal cortex in VWM remains unclear. Some evidence (Paulesu et al., 1993; Awh et al., 1996) suggests that the parietal cortex mediates the storage of verbal information, but these studies and most previous ones included encoding and retrieval processes as well as storage and rehearsal of verbal information. A recent positron emission tomography (PET) study by Fiez et al. (1996) isolated storage and rehearsal from other VWM processes and did not find reliable activation in parietal cortex. This result suggests that parietal cortex may not be involved in VWM storage, contrary to previous proposals. However, we report two behavioral studies indicating that some of the verbal material used by Fiez et al. (1996) may not have required phonological representations in VWM. In addition, we report a PET study that isolated VWM encoding, retrieval, and storage and rehearsal processes in different PET scans and used material likely to require phonological codes in VWM. After subtraction of appropriate controls, the encoding condition revealed no reliable activations; the retrieval condition revealed reliable activations in dorsolateral prefrontal, anterior cingulate, posterior parietal, and extrastriate cortices, and the storage condition revealed reliable activations in dorsolateral prefrontal, inferior frontal, premotor, and posterior parietal cortices, as well as cerebellum. These results suggest that parietal regions are part of a network of brain areas that mediate the short-term storage and retrieval of phonologically coded verbal material.', 'venue': 'Journal of Neuroscience', 'year': 1998, 'authors': [{'authorId': '1869771', 'name': 'J. Jonides'}, {'authorId': '39363308', 'name': 'E. Schumacher'}, {'authorId': '144541931', 'name': 'Edward E. Smith'}, {'authorId': '1795763', 'name': 'R. Koeppe'}, {'authorId': '2889676', 'name': 'E. Awh'}, {'authorId': '1398013984', 'name': 'P. Reuter-Lorenz'}, {'authorId': '5359696', 'name': 'C. Marshuetz'}, {'authorId': '2131122188', 'name': 'Christopher R. Willis'}]}, {'paperId': '9b1ccecf2b9769479be244cd70616f41b8e28b1e', 'title': 'Associative Document Retrieval Techniques Using Bibliographic Information', 'abstract': 'Automatic documentation systems which use the words contained in the individual documents as a principal source of document identifications may not perform satisfactorily under all circumstances. Methods have therefore been devised within the last few years for computing association measures between words and between documents, and for using such associated words, or information contained in associated documents, to supplement and refine the original document identifications. I t is suggested in this study that bibliographic citations may provide a simple means for obtaining associated documents to be incorporated in an automatic documentation system. The standard associative retrieval techniques are first briefly reviewed. A computer experiment is then described which tends to confirm the hypothesis that documents exhibiting similar citation sets also deal with similar subject matter. Finally, a fully automatic document retrieval system is proposed which uses bibliographic information in addition to other standard criteria for the identification of document content, and for the detection of relevant information.', 'venue': 'JACM', 'year': 1963, 'authors': [{'authorId': '1797808', 'name': 'G. Salton'}]}, {'paperId': '686ac5ea550e8b73c98c988c420c41a7833ca844', 'title': 'Learning Distance Metrics with Contextual Constraints for Image Retrieval', 'abstract': 'Relevant Component Analysis (RCA) has been proposed for learning distance metrics with contextual constraints for image retrieval. However, RCA has two important disadvantages. One is the lack of exploiting negative constraints which can also be informative, and the other is its incapability of capturing complex nonlinear relationships between data instances with the contextual information. In this paper, we propose two algorithms to overcome these two disadvantages, i.e., Discriminative Component Analysis (DCA) and Kernel DCA. Compared with other complicated methods for distance metric learning, our algorithms are rather simple to understand and very easy to solve. We evaluate the performance of our algorithms on image retrieval in which experimental results show that our algorithms are effective and promising in learning good quality distance metrics for image retrieval.', 'venue': 'Computer Vision and Pattern Recognition', 'year': 2006, 'authors': [{'authorId': '1741126', 'name': 'S. Hoi'}, {'authorId': '46641573', 'name': 'W. Liu'}, {'authorId': '1785083', 'name': 'Michael R. Lyu'}, {'authorId': '1712167', 'name': 'Wei-Ying Ma'}]}, {'paperId': '036245d3902967c8ec6605f6d97fded0a7d1d332', 'title': 'Effective information retrieval using genetic algorithms based matching functions adaptation', 'abstract': 'Knowledge intensive organizations have vast array of information contained in large document repositories. With the advent of E-commerce and corporate intranets/extranets, these repositories are expected to grow at a fast pace. This explosive growth has led to huge, fragmented, and unstructured document collections. Although it has become easier to collect and store information in document collections, it has become increasingly difficult to retrieve relevant information from these large document collections. This paper addresses the issue of improving retrieval performance (in terms of precision and recall) for retrieval from document collections. There are three important paradigms of research in the area of information retrieval (1R): Probabilistic IR, Knowledge-based IR, and, Artificial Intelligence based techniques like neural networks and symbolic learning. Very few researcher have tried to use evolutionary algorithms like genetic algorithms (GAs). Previous attempts at using GAs have concentrated on modifying document representations or modifying query representations. This work looks at the possibility of applying GAs to adapt various matching functions. It is hoped that such an adaptation of the matching functions in lead to a better retrieval performance than that obtained by using a single matching function. An overall matching function is treated as an weighted combination of scores produced by individual matching functions. This overall score is asked to rank and retrieve documents. Weights associated with individual functions are searched using Genetic Algorithms. The idea is tested on a real document collection called the Cranfield collection. The results look very encouraging.', 'venue': 'Proceedings of the Annual Hawaii International Conference on System Sciences', 'year': 2000, 'authors': [{'authorId': '144340728', 'name': 'Praveen Pathak'}, {'authorId': '20094419', 'name': 'Michael D. Gordon'}, {'authorId': '145631869', 'name': 'Weiguo Fan'}]}, {'paperId': 'a84ebf0f08bd56d41e3f4dce3d3a101336da7173', 'title': 'LyberWorld—a visualization user interface supporting fulltext retrieval', 'abstract': None, 'venue': 'Annual International ACM SIGIR Conference on Research and Development in Information Retrieval', 'year': 1994, 'authors': [{'authorId': '1799848', 'name': 'M. Hemmje'}, {'authorId': '2348438', 'name': 'C. Kunkel'}, {'authorId': '2071537401', 'name': 'Alexander Willett'}]}, {'paperId': '3bb44af2251b89c83a922778be5ba50cc205cc8c', 'title': 'Algorithmic mediation for collaborative exploratory search', 'abstract': \"We describe a new approach to information retrieval: algorithmic mediation for intentional, synchronous collaborative exploratory search. Using our system, two or more users with a common information need search together, simultaneously. The collaborative system provides tools, user interfaces and, most importantly, algorithmically-mediated retrieval to focus, enhance and augment the team's search and communication activities. Collaborative search outperformed post hoc merging of similarly instrumented single user runs. Algorithmic mediation improved both collaborative search (allowing a team of searchers to find relevant information more efficiently and effectively), and exploratory search (allowing the searchers to find relevant information that cannot be found while working individually).\", 'venue': 'Annual International ACM SIGIR Conference on Research and Development in Information Retrieval', 'year': 2008, 'authors': [{'authorId': '143998882', 'name': 'Jeremy Pickens'}, {'authorId': '2724097', 'name': 'G. Golovchinsky'}, {'authorId': '144797719', 'name': 'C. Shah'}, {'authorId': '2826123', 'name': 'P. Qvarfordt'}, {'authorId': '2122588', 'name': 'Maribeth Back'}]}, {'paperId': '4025db5bd25f6ce888cd22a6c73d497045c7b00f', 'title': 'A Generalized Term Dependence Model in Information Retrieval', 'abstract': 'The tree dependence model has been used successfully to incorporate dependencies between certain term pairs on the information retrieval process, while the Bahadur Lazarsfeld Expansion (BLE) which specifies dependencies between all subsets of terms has been used to identify productive clusters of items in a clustered data base environment. The successes of these models are unlikely to be accidental; it is of interest therefore to examine the similarities between the two models. The disadvantage of the BLE model is the exponential number of terms appearings in the full expression, while a truncated BLE system may produce negative probability values. The disadvantage of the tree dependence model is the restriction to dependencies between certain term pairs only and the exclusion of higher-order dependencies. A generalized term dependence model is introduced in this study which does not carry the disadvantages of either the tree dependence or the BLE models. Sample evaluation results are included to demonstrate the usefulness of the generalized system.', 'venue': '', 'year': 1983, 'authors': [{'authorId': '2107333339', 'name': 'C. T. Yu'}, {'authorId': '144009691', 'name': 'C. Buckley'}, {'authorId': '145569384', 'name': 'K. Lam'}, {'authorId': '1797808', 'name': 'G. Salton'}]}, {'paperId': '3cde32d8ee26d6c5fbc29989831190c8dd2f6af7', 'title': 'Effects of Stop Words Elimination for Arabic Information Retrieval: A Comparative Study', 'abstract': 'The effectiveness of three stop words lists for Arabic Information Retrieval---General Stoplist, Corpus- Based Stoplist, Combined Stoplist ---were investigated in this study. Three popular weighting schemes were examined: the inverse document frequency weight, probabilistic weighting, and statistical language modelling. The Idea is to combine the statistical approaches with linguistic approaches to reach an optimal performance, and compare their effect on retrieval. The LDC (Linguistic Data Consortium) Arabic Newswire data set was used with the Lemur Toolkit. The Best Match weighting scheme used in the Okapi retrieval system had the best overall performance of the three weighting algorithms used in the study, stoplists improved retrieval effectiveness especially when used with the BM25 weight. The overall performance of a general stoplist was better than the other two lists.', 'venue': 'ArXiv', 'year': 2017, 'authors': [{'authorId': '1403987068', 'name': 'I. A. El-Khair'}]}, {'paperId': '6dbe3cfdb62cb0a67c53ce6cdc159a8a33d3e2a8', 'title': 'Health information on the Internet: gold mine or minefield?', 'abstract': 'The Internet has revolutionized the way information is shared and accessed. Information retrieval is easier now than ever before. Since the rise of modern search engines, social networks, and ubiquitous access through devices such as smartphones and tablet or laptop computers, information is', 'venue': 'Canadian family physician Medecin de famille canadien', 'year': 2014, 'authors': [{'authorId': '49279828', 'name': 'Tabitha Tonsaker'}, {'authorId': '50664327', 'name': 'G. Bartlett'}, {'authorId': '48945036', 'name': 'C. Trpkov'}]}, {'paperId': '87e804bef28d4f22c73671e9933f8a020e9235bb', 'title': 'Linguistically Motivated Information Retrieval', 'abstract': None, 'venue': '', 'year': 2000, 'authors': [{'authorId': '35575984', 'name': 'A. Arampatzis'}, {'authorId': '2149432', 'name': 'T. V. D. Weide'}, {'authorId': '1731326', 'name': 'P. V. Bommel'}, {'authorId': '1713642', 'name': 'C. Koster'}]}, {'paperId': 'a77372f6b3f320cbd81b88f7c02024eb2d945efa', 'title': 'The Ordered Weighted Averaging Operators: Theory and Applications', 'abstract': \"This volume is the first in the literature on the increasingly popular Ordered Weighted Averaging (OWA) operators. These OWA operators make it possible to change the form of aggregation from the `pessimistic' minimum-type aggregation through all intermediate types including the conventional arithmetic mean and nonconventional aggregations, to the `optimistic' maximum-type aggregations. Included are contributions from a number of fields where these operators have been applied. These fields are decision analysis under uncertainty, learning and classification, multi-person decision-making and consensus formation, and flexible database querying and information retrieval.\", 'venue': '', 'year': 1997, 'authors': [{'authorId': '144127749', 'name': 'R. Yager'}, {'authorId': '1760085', 'name': 'J. Kacprzyk'}]}, {'paperId': 'cdd7ef595e56c6a3ce51770ad6e442479dcc5aa6', 'title': 'Knowledge-Based Navigation of Complex Information Spaces', 'abstract': 'While the explosion of on-line information has brought new opportunities for finding and using electronic data, it has also brought to the forefront the problem of isolating useful information and making sense of large multidimension information spaces. We have built several developed an approach to building data \"tour guides,\" called FINDME systems. These programs know enough about an information space to be able to help a user navigate through it. The user not only comes away with items of useful information but also insights into the structure of the information space itself. In these systems, we have combined ideas of instance-based browsing, structuring retrieval around the critiquing of previously-retrieved examples, and retrieval strategies, knowledge-based heuristics for finding relevant information. We illustrate these techniques with several examples, concentrating especially on the RENTME system, a FINDME system for helping users find suitable rental apartments in the Chicago metropolitan area.', 'venue': 'AAAI/IAAI, Vol. 1', 'year': 1996, 'authors': [{'authorId': '1747150', 'name': 'R. Burke'}, {'authorId': '2263239', 'name': 'K. Hammond'}, {'authorId': '2052481823', 'name': 'Benjamin C. Young'}]}, {'paperId': '6fec6b19dd0b69f6d77481fd351028ad8532b752', 'title': 'Information Retrieval and Criticality in Parity-Time-Symmetric Systems.', 'abstract': 'By investigating information flow between a general parity-time (PT-)symmetric non-Hermitian system and an environment, we find that the complete information retrieval from the environment can be achieved in the PT-unbroken phase, whereas no information can be retrieved in the PT-broken phase. The PT-transition point thus marks the reversible-irreversible criticality of information flow, around which many physical quantities such as the recurrence time and the distinguishability between quantum states exhibit power-law behavior. Moreover, by embedding a PT-symmetric system into a larger Hilbert space so that the entire system obeys unitary dynamics, we reveal that behind the information retrieval lies a hidden entangled partner protected by PT symmetry. Possible experimental situations are also discussed.', 'venue': 'Physical Review Letters', 'year': 2017, 'authors': [{'authorId': '122766796', 'name': 'K. Kawabata'}, {'authorId': '6632753', 'name': 'Y. Ashida'}, {'authorId': '2815318', 'name': 'Masahito Ueda'}]}, {'paperId': 'cd39299da14e8ca15983f82c5978b0c4b2e18c3e', 'title': 'The Capacity of Private Information Retrieval With Partially Known Private Side Information', 'abstract': 'We consider the problem of private information retrieval (PIR) of a single message out of <inline-formula> <tex-math notation=\"LaTeX\">$K$ </tex-math></inline-formula> messages from <inline-formula> <tex-math notation=\"LaTeX\">$N$ </tex-math></inline-formula> replicated and non-colluding databases where a cache-enabled user (retriever) of cache-size <inline-formula> <tex-math notation=\"LaTeX\">$M$ </tex-math></inline-formula> possesses side information in the form of full messages that are partially known to the databases. In this model, the user and the databases engage in a two-phase scheme, namely, the prefetching phase where the user acquires side information and the retrieval phase where the user downloads desired information. In the prefetching phase, the user receives <inline-formula> <tex-math notation=\"LaTeX\">$m_{n}$ </tex-math></inline-formula> full messages from the <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula>th database, under the cache memory size constraint <inline-formula> <tex-math notation=\"LaTeX\">$\\\\sum _{n=1}^{N} m_{n} \\\\leq M$ </tex-math></inline-formula>. In the retrieval phase, the user wishes to retrieve a message (which is not present in its memory) such that no individual database learns anything about the identity of the desired message. In addition, the identities of the side information messages that the user did not prefetch from a database must remain private against that database. Since the side information provided by each database in the prefetching phase is known by the providing database and the side information must be kept private against the remaining databases, we coin this model as <italic>partially known private side information</italic>. We characterize the capacity of the PIR with partially known private side information to be <inline-formula> <tex-math notation=\"LaTeX\">$C=\\\\left ({1+\\\\frac {1}{N}+\\\\cdots +\\\\frac {1}{N^{K-M-1}}}\\\\right)^{-1}=\\\\frac {1-\\\\frac {1}{N}}{1-\\\\left({\\\\frac {1}{N}}\\\\right)^{K-M}}$ </tex-math></inline-formula>. Interestingly, this result is the same if none of the databases knows any of the prefetched side information, i.e., when the side information is obtained externally, a problem posed by Kadhe et al. and settled by Chen-Wang-Jafar recently. Thus, our result implies that there is no loss in using the same databases for both prefetching and retrieval phases.', 'venue': 'IEEE Transactions on Information Theory', 'year': 2017, 'authors': [{'authorId': '1947286', 'name': 'Yi-Peng Wei'}, {'authorId': '1995965', 'name': 'Karim A. Banawan'}, {'authorId': '1744394', 'name': 'S. Ulukus'}]}, {'paperId': '67560f2239a1e1b6a997a71c27f7199533ca4374', 'title': 'Query Formulation as an Information Retrieval Problem', 'abstract': 'Query formulation in the context of large conceptual schemata is known to be a hard problem. When formulating ad hoc queries users may become overwhelmed by the vast amount of information that is stored in the information system; leading to a feeling of lost in conceptual space. In this article we develop a strategy to cope with this problem. This strategy is based on ideas from the information retrieval world, in particular the query by navigation mechanism and the stratified hypermedia architecture. The stratified hypermedia architecture is used to describe the information contained in the information system on multiple levels of abstraction. When using our approach to the formulation of queries, a user will first formulate a number of simple queries corresponding to linear paths through the information structure. The formulation of the linear paths is the result of the explorative phase of query formulation. Once users have specified a number of these linear paths, they may combine them to form more complex queries. This last process is referred to as query by construction and corresponds to the constructive phase of the query formulation process.', 'venue': 'Computer/law journal', 'year': 1996, 'authors': [{'authorId': '143613819', 'name': 'A. Hofstede'}, {'authorId': '1719743', 'name': 'H. Proper'}, {'authorId': '2149432', 'name': 'T. V. D. Weide'}]}, {'paperId': 'b68a6498592d723941ce7c257f392388c9bde779', 'title': 'A Maximum Likelihood Ratio Information Retrieval Model', 'abstract': 'Abstract : In this paper we present a novel probabilistic information retrieval model that scores documents based on the relative change in the document likelihoods, expressed as the ratio of the conditional probability of the document given the query and the prior probability of the document before the query is specified. The document likelihoods are computed using statistical language modeling techniques and the model parameters are estimated automatically and dynamically for each query to optimize well-specified (maximum likelihood) objective functions. We derive the basic retrieval model, describe the details of the model, and present some extensions to the model including a method to perform automatic feedback. Development experiments are performed using the TREC-6 ad hoc text retrieval task and performance is measured using the TREC-7 ad hoc task. Official evaluation results on the 1999 TREC-8 ad hoc task are also reported. The performance results demonstrate that the model is competitive with current state-of-the-art retrieval approaches.', 'venue': 'Text Retrieval Conference', 'year': 1999, 'authors': [{'authorId': '40446176', 'name': 'Kenney Ng'}]}, {'paperId': '36108977c66a2e45e29fde2500621902ab108e78', 'title': 'Information sharing in academic communities: Types and levels of collaboration in information seeking and use.', 'abstract': 'Research and theories of information behavior have traditionally focused on the \"information man,\" i.e., on the individual as a seeker and user of information. The collective aspects of information behavior have been conceptualized, for instance, as consulting, informal seeking, use of person sources, and peer influence. These conceptualizations suggest a one-way process in which an individual consults another individual, however, information acquisition and filtering often is undertaken as a collective and collaborative effort. The paper develops a conceptual framework for the description of types and levels of information sharing in relation to document retrieval in academic communities. The concepts of strategic information sharing, paradigmatic information sharing, directive information sharing, and social information sharing are introduced to describe differences in the goals and purposes of information sharing in different groups and contexts of interaction.', 'venue': '', 'year': 2002, 'authors': [{'authorId': '2014642', 'name': 'Sanna Talja'}]}, {'paperId': '408f9a6585db6f34f9c724b0f5c3600f0e9dab24', 'title': 'Latent Semantic Indexing', 'abstract': 'Latent semantic indexing (LSI) is an information retrieval technique based on the spectral analysis of the term-document matrix, whose empirical success had heretofore been without rigorous prediction and explanation. We prove that, under certain conditions, LSI does succeed in capturing the underlying semantics of the corpus and achieves improved retrieval performance. We propose the technique of random projection as a way of speeding up LSI. We complement our theorems with encouraging experimental results. We also argue that our results may be viewed in a more general framework, as a theoretical basis for the use of spectral methods in a wider class of applications such as collaborative filtering.', 'venue': 'ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems', 'year': 2000, 'authors': [{'authorId': '144102674', 'name': 'C. Papadimitriou'}, {'authorId': '145503401', 'name': 'P. Raghavan'}, {'authorId': '145955092', 'name': 'H. Tamaki'}, {'authorId': '1737804', 'name': 'S. Vempala'}]}, {'paperId': '2175dafeb0a9e6965da72bc0849111eff0da16f1', 'title': 'Multilingual Information Retrieval', 'abstract': None, 'venue': 'Springer Berlin Heidelberg', 'year': 2012, 'authors': [{'authorId': '144423157', 'name': 'C. Peters'}, {'authorId': '3075644', 'name': 'Martin Braschler'}, {'authorId': '1704149', 'name': 'Paul D. Clough'}]}, {'paperId': 'b8570c6d9f9670f4a864b275f021af9c404e3070', 'title': 'Towards more accurate retrieval of duplicate bug reports', 'abstract': 'In a bug tracking system, different testers or users may submit multiple reports on the same bugs, referred to as duplicates, which may cost extra maintenance efforts in triaging and fixing bugs. In order to identify such duplicates accurately, in this paper we propose a retrieval function (REP) to measure the similarity between two bug reports. It fully utilizes the information available in a bug report including not only the similarity of textual content in summary and description fields, but also similarity of non-textual fields such as product, component, version, etc. For more accurate measurement of textual similarity, we extend BM25F - an effective similarity formula in information retrieval community, specially for duplicate report retrieval. Lastly we use a two-round stochastic gradient descent to automatically optimize REP for specific bug repositories in a supervised learning manner. We have validated our technique on three large software bug repositories from Mozilla, Eclipse and OpenOffice. The experiments show 10–27% relative improvement in recall rate@k and 17–23% relative improvement in mean average precision over our previous model. We also applied our technique to a very large dataset consisting of 209,058 reports from Eclipse, resulting in a recall rate@k of 37–71% and mean average precision of 47%.', 'venue': 'International Conference on Automated Software Engineering', 'year': 2011, 'authors': [{'authorId': '34751114', 'name': 'Chengnian Sun'}, {'authorId': '143960553', 'name': 'D. Lo'}, {'authorId': '144195334', 'name': 'Siau-Cheng Khoo'}, {'authorId': '144924150', 'name': 'Jing Jiang'}]}, {'paperId': '819fe6a7f20e6d3fe84673acc550d450609db5d7', 'title': 'The impact of database selection on distributed searching', 'abstract': 'The proliferation of online information resources increases the importance of effective and efficient distributed searching. Distributed searching is cast in three parts — database selection, query processing, and results merging. In this paper we examine the effect of database selection on retrieval performance. We look at retrieval performance in three different distributed retrieval testbeds and distill some general results. First we find that good database selection can result in better retrieval effectiveness than can be achieved in a centralized database. Second we find that good performance can be achieved when only a few sites are selected and that the performance generally increases as more sites are selected. Finally we find that when database selection is employed, it is not necessary to maintain collection wide information (CWI), e.g. global idf. Local information can be used to achieve superior performance. This means that distributed systems can be engineered with more autonomy and less cooperation. This work suggests that improvements in database selection can lead to broader improvements in retrieval performance, even in centralized (i.e. single database) systems. Given a centralized database and a good selection mechanism, retrieval performance can be improved by decomposing that database conceptually and employing a selection step.', 'venue': 'Annual International ACM SIGIR Conference on Research and Development in Information Retrieval', 'year': 2000, 'authors': [{'authorId': '33968184', 'name': 'Allison L. Powell'}, {'authorId': '1713791', 'name': 'J. French'}, {'authorId': '144987107', 'name': 'Jamie Callan'}, {'authorId': '1735186', 'name': 'Margaret E. Connell'}, {'authorId': '1679482', 'name': 'C. Viles'}]}, {'paperId': '01019fcbf466dda8dc514ecf738a679f24033abc', 'title': 'Evaluation of information retrieval systems : Approaches, issues, and methods', 'abstract': None, 'venue': '', 'year': 1997, 'authors': [{'authorId': '34986152', 'name': 'S. P. Harter'}, {'authorId': '2128399', 'name': 'Carol A. Hert'}]}, {'paperId': '7c3664f31c6a27cd715409af9eaf81d39766f990', 'title': 'The Random Projection Method', 'abstract': 'Random projection Combinatorial optimization: Rounding via random projection Embedding metrics in Euclidean space Euclidean embeddings: Beyond distance preservation Learning theory: Robust concepts Intersections of half-spaces Information retrieval: Nearest neighbors Indexing and clustering Bibliography Appendix.', 'venue': 'DIMACS Series in Discrete Mathematics and Theoretical Computer Science', 'year': 2005, 'authors': [{'authorId': '1737804', 'name': 'S. Vempala'}]}, {'paperId': '355b86dafd852e4df905f6ad9402c7d03831d618', 'title': 'Probabilistic Latent Semantic Analysis', 'abstract': 'Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments.', 'venue': 'Conference on Uncertainty in Artificial Intelligence', 'year': 1999, 'authors': [{'authorId': '143936663', 'name': 'Thomas Hofmann'}]}, {'paperId': '9ba2572efdbb004305f710584a4804dd6380d729', 'title': 'LifeLogging: Personal Big Data', 'abstract': \"We have recently observed a convergence of technologies to foster the emergence of lifelogging as a mainstream activity. Computer storage has become significantly cheaper, and advancements in sensing technology allows for the efficient sensing of personal activities, locations and the environment. This is best seen in the growing popularity of the quantified self movement, in which life activities are tracked using wearable sensors in the hope of better understanding human performance in a variety of tasks. This review aims to provide a comprehensive summary of lifelogging, to cover its research history, current technologies, and applications. Thus far, most of the lifelogging research has focused predominantly on visual lifelogging, hence we maintain this focus in this review. However, we also reflect on the challenges lifelogging poses for information access and retrieval in general. This review is a suitable reference for those seeking an information retrieval scientist's perspective on lifelogging and the quantified self.\", 'venue': 'Foundations and Trends in Information Retrieval', 'year': 2014, 'authors': [{'authorId': '1737981', 'name': 'C. Gurrin'}, {'authorId': '1680223', 'name': 'A. Smeaton'}, {'authorId': '2939042', 'name': 'A. Doherty'}]}, {'paperId': '48dd598b5b43181f430fd983a66eec689cb9ab9e', 'title': 'Using User Models in Music Information Retrieval Systems', 'abstract': 'To make multimedia data easily retrieved, we use metadata to describe the information, so that search engines or other information filter tools can effectively and efficiently locate and retrieve the multimedia content. Since many features of multimedia content are perceptual and user-dependent, user modeling is also necessary for multimedia information retrieval systems, e.g., music information retrieval systems. Furthermore, to make the user models sharable, we need standardized language to describe them. In this paper, an XMLlike language is proposed to describe the user model for music information retrieval purposes. We also propose some paradigms to acquire, deploy and share the user information to improve current music information systems. A prototype system, MusicCat, is analyzed and implemented as a case.', 'venue': 'International Society for Music Information Retrieval Conference', 'year': 2000, 'authors': [{'authorId': '2055400084', 'name': 'Wei Chai'}, {'authorId': '9069248', 'name': 'B. Vercoe'}]}, {'paperId': 'd6d5d4bb3c805b9e3bd3cb0b845d8a05eb391bd3', 'title': 'Interdisciplinary Communities and Research Issues in Music Information Retrieval', 'abstract': 'Music Information Retrieval (MIR) is an interdisciplinary research area that has grown out of the need to manage burgeoning collections of music in digital form. Its diverse disciplinary communities have yet to articulate a common research agenda or agree on methodological principles and metrics of success. In order for MIR to succeed, researchers need to work with real user communities and develop research resources such as reference music collections, so that the wide variety of techniques being developed in MIR can be meaningfully compared with one another. Out of these efforts, a common MIR practice can emerge.', 'venue': 'International Society for Music Information Retrieval Conference', 'year': 2002, 'authors': [{'authorId': '2518189', 'name': 'J. Futrelle'}, {'authorId': '145656248', 'name': 'J. S. Downie'}]}, {'paperId': 'bff0583ca4db2c07f300037bd5181822c7fd6461', 'title': 'Who said what? An event-related potential investigation of source and item memory.', 'abstract': 'Event-related potentials (ERPs) were recorded during recognition tasks for spoken words alone (items) or for both words and the voice of the speaker (sources). Neither performance nor ERP measures suggested that voice information was retrieved automatically during the item-recognition task. In both tasks, correctly recognized old words elicited more positive ERPs than new words, beginning around 400 ms poststimulus onset. In the source task only, old words also elicited a focal prefrontal positivity beginning about 700 ms. The prefrontal task effect did not distinguish trials with accurate and inaccurate voice judgments and is interpreted as reflecting the search for voice information in memory. More posterior recording sites were sensitive to the successful recovery of voice or source information. The results indicate that word and voice information were retrieved hierarchically and distinguish retrieval attempt from retrieval success.', 'venue': 'Journal of Experimental Psychology. Learning, Memory and Cognition', 'year': 1998, 'authors': [{'authorId': '6789615', 'name': 'A. Senkfor'}, {'authorId': '117919308', 'name': 'C. van Petten'}]}, {'paperId': 'e75cae50cd9e12be6675e994cc6a122b806ce6ab', 'title': 'Memory Factors in Advertising: The Effect of Advertising Retrieval Cues on Brand Evaluations', 'abstract': \"Because consumers may not make brand decisions during ad exposure, consumer memory for advertising is important. Yet, the cues typically available for brand decisions, such as the brand name, may not effectively retrieve the information, thoughts, or feelings stored in memory from prior ad exposure. A laboratory experiment showed that advertising retrieval cues (i.e., other verbal or visual information from the ad) facilitated access of elements from the ad memory trace and affected brand evaluations. Two other factors, competitive ad interference (i.e., the number of competing brands advertising within a product category) and consumers' processing goals during ad exposure, also affected consumer ad memory and brand evaluations.\", 'venue': '', 'year': 1987, 'authors': [{'authorId': '4864052', 'name': 'Kevin Lane Keller'}]}, {'paperId': '28d9f9cb94b1295fb2f03103a0c4d2eb72298445', 'title': 'Global Text Matching for Information Retrieval', 'abstract': 'An approach is outlined for the retrieval of natural language texts in response to available search requests and for the recognition of content similarities between text excerpts. The proposed retrieval process is based on flexible text matching procedures carried out in a number of different text environments and is applicable to large text collections covering unrestricted subject matter. For unrestricted text environments this system appears to outperform other currently available methods.', 'venue': 'Science', 'year': 1991, 'authors': [{'authorId': '1797808', 'name': 'G. Salton'}, {'authorId': '144009691', 'name': 'C. Buckley'}]}, {'paperId': '6633814eb1e066d146bdae16d0c1c8344c60778c', 'title': 'Corpus-based and Knowledge-based Measures of Text Semantic Similarity', 'abstract': 'This paper presents a method for measuring the semantic similarity of texts, using corpus-based and knowledge-based measures of similarity. Previous work on this problem has focused mainly on either large documents (e.g. text classification, information retrieval) or individual words (e.g. synonymy tests). Given that a large fraction of the information available today, on the Web and elsewhere, consists of short text snippets (e.g. abstracts of scientific documents, imagine captions, product descriptions), in this paper we focus on measuring the semantic similarity of short texts. Through experiments performed on a paraphrase data set, we show that the semantic similarity method out-performs methods based on simple lexical matching, resulting in up to 13% error rate reduction with respect to the traditional vector-based similarity metric.', 'venue': 'AAAI Conference on Artificial Intelligence', 'year': 2006, 'authors': [{'authorId': '145557251', 'name': 'Rada Mihalcea'}, {'authorId': '1947728', 'name': 'C. Corley'}, {'authorId': '1723976', 'name': 'C. Strapparava'}]}, {'paperId': '08be57e469c8899dc4be09445aa2472fd0b50dd3', 'title': 'Multi-index hashing for information retrieval', 'abstract': 'We describe a technique for building hash indices for a large dictionary of strings. This technique permits robust retrieval of strings from the dictionary even when the query pattern has a significant number of errors. This technique is closely related to the classical Turan problem for hypergraphs. We propose a general method of multi-index construction by generalizing certain Turan hypergraphs. We also develop an accompanying theory for analyzing such hashing schemes. The resulting algorithms have been implemented and can be applied to a wide variety of recognition and retrieval problems.<<ETX>>', 'venue': 'Proceedings 35th Annual Symposium on Foundations of Computer Science', 'year': 1994, 'authors': [{'authorId': '2422263', 'name': 'D. Greene'}, {'authorId': '3026032', 'name': 'Michal Parnas'}, {'authorId': '145874305', 'name': 'F. F. Yao'}]}, {'paperId': 'acc10d31aad7ebff7f9cbe47a966aa4c9e3310d7', 'title': 'Concept data analysis - theory and applications', 'abstract': 'Foreword.Preface.I: THEORY AND ALGORITHMS.1. Theoretical Foundations.1.1 Basic Notions of Orders and Lattices.1.2 Context, Concept, and Concept Lattice.1.3 Many-valued Contexts.1.4 Bibliographic Notes.2. Algorithms.2.1 Constructing Concept Lattices.2.2 Incremental Lattice Update.2.3 Visualization.2.4 Adding Knowledge to Concept Lattices.2.5 Bibliographic Notes.II: APPLICATIONS.3. Information Retrieval.3.1 Query Modification.3.2 Document Ranking4. Text Mining.4.1 Mining the Content of the ACM Digital Library.4.2 MiningWeb Retrieval Results with CREDO.4.3 Bibliographic Notes.5. Rule Mining.5.1 Implications.5.2 Functional Dependencies.5.3 Association Rules.5.4 Classification Rules.5.5 Bibliographic Notes.References.Index.', 'venue': '', 'year': 2004, 'authors': [{'authorId': '1701045', 'name': 'Claudio Carpineto'}, {'authorId': '144857383', 'name': 'Giovanni Romano'}]}, {'paperId': 'a9125e01372dea7be662c420447b8b482af0b77a', 'title': 'Retrieval and browsing of spoken content', 'abstract': \"Ever-increasing computing power and connectivity bandwidth, together with falling storage costs, are resulting in an overwhelming amount of data of various types being produced, exchanged, and stored. Consequently, information search and retrieval has emerged as a key application area. Text-based search is the most active area, with applications that range from Web and local network search to searching for personal information residing on one's own hard-drive. Speech search has received less attention perhaps because large collections of spoken material have previously not been available. However, with cheaper storage and increased broadband access, there has been a subsequent increase in the availability of online spoken audio content such as news broadcasts, podcasts, and academic lectures. A variety of personal and commercial uses also exist. As data availability increases, the lack of adequate technology for processing spoken documents becomes the limiting factor to large-scale access to spoken content. In this article, we strive to discuss the technical issues involved in the development of information retrieval systems for spoken audio documents, concentrating on the issue of handling the errorful or incomplete output provided by ASR systems. We focus on the usage case where a user enters search terms into a search engine and is returned a collection of spoken document hits.\", 'venue': 'IEEE Signal Processing Magazine', 'year': 2008, 'authors': [{'authorId': '1802969', 'name': 'Ciprian Chelba'}, {'authorId': '1798550', 'name': 'Timothy J. Hazen'}, {'authorId': '2535627', 'name': 'M. Saraçlar'}]}, {'paperId': '7d38cef9247b1f00f6130c05a09e14b8c6ee01bd', 'title': '\"Isms\" in information science: constructivism, collectivism and constructionism', 'abstract': \"Purpose – Describes the basic premises of three metatheories that represent important or emerging perspectives on information seeking, retrieval and knowledge formation in information science: constructivism, collectivism, and constructionism.Design/methodology/approach – Presents a literature‐based conceptual analysis. Pinpoints the differences between the positions in their conceptions of language and the nature and origin of knowledge.Findings – Each of the three metatheories addresses and solves specific types of research questions and design problems. The metatheories thus complement one another. Each of the three metatheories encourages and constitutes a distinctive type of research and learning.Originality/value – Outlines each metatheory's specific fields of application.\", 'venue': 'J. Documentation', 'year': 2005, 'authors': [{'authorId': '2014642', 'name': 'Sanna Talja'}, {'authorId': '34846180', 'name': 'K. Tuominen'}, {'authorId': '3141223', 'name': 'Reijo Savolainen'}]}, {'paperId': 'd0a68ae4fa57f1f89271c433dc86b2b85bf2675f', 'title': 'General constructions for information-theoretic private information retrieval', 'abstract': None, 'venue': 'Journal of computer and system sciences (Print)', 'year': 2005, 'authors': [{'authorId': '1729686', 'name': 'A. Beimel'}, {'authorId': '1688856', 'name': 'Y. Ishai'}, {'authorId': '1738470', 'name': 'E. Kushilevitz'}]}, {'paperId': 'bfde0f9190d5b0a5d4302d5d267cbb2ef7566c5f', 'title': 'The contribution of the human posterior parietal cortex to episodic memory', 'abstract': None, 'venue': 'Nature Reviews Neuroscience', 'year': 2017, 'authors': [{'authorId': '2460674', 'name': 'C. Sestieri'}, {'authorId': '39269549', 'name': 'G. Shulman'}, {'authorId': '1723344', 'name': 'M. Corbetta'}]}, {'paperId': '0bf0c4e7c4d2ec675c7b68a78266c3dd9a31ba7e', 'title': 'Latent topic feedback for information retrieval', 'abstract': 'We consider the problem of a user navigating an unfamiliar corpus of text documents where document metadata is limited or unavailable, the domain is specialized, and the user base is small. These challenging conditions may hold, for example, within an organization such as a business or government agency. We propose to augment standard keyword search with user feedback on latent topics. These topics are automatically learned from the corpus in an unsupervised manner and presented alongside search results. User feedback is then used to reformulate the original query, resulting in improved information retrieval performance in our experiments.', 'venue': 'Knowledge Discovery and Data Mining', 'year': 2011, 'authors': [{'authorId': '34665428', 'name': 'David Andrzejewski'}, {'authorId': '1690267', 'name': 'David J. Buttler'}]}, {'paperId': '4ca759b11fbdfdd8c409588bf258b250b1e7fdcd', 'title': 'A context vector model for information retrieval', 'abstract': 'In the vector space model for information retrieval, term vectors are pair-wise orthogonal, that is, terms are assumed to be independent. It is well known that this assumption is too restrictive. In this article, we present our work on an indexing and retrieval method that, based on the vector space model, incorporates term dependencies and thus obtains semantically richer representations of documents. First, we generate term context vectors based on the co-occurrence of terms in the same documents. These vectors are used to calculate context vectors for documents. We present different techniques for estimating the dependencies among terms. We also define term weights that can be employed in the model. Experimental results on four text collections (MED, CRANFIELD, CISI, and CACM) show that the incorporation of term dependencies in the retrieval process performs statistically significantly better than the classical vector space model with IDF weights. We also show that the degree of semantic matching versus direct word matching that performs best varies on the four collections. We conclude that the model performs well for certain types of queries and, generally, for information tasks with high recall requirements. Therefore, we propose the use of the context vector model in combination with other, direct word-matching methods.', 'venue': 'J. Assoc. Inf. Sci. Technol.', 'year': 2002, 'authors': [{'authorId': '1725454', 'name': 'H. Billhardt'}, {'authorId': '1788554', 'name': 'D. Borrajo'}, {'authorId': '1703526', 'name': 'V. Maojo'}]}, {'paperId': '12b561213409ff14f39d78070f417b1895034a75', 'title': 'Mobile Medical Visual Information Retrieval', 'abstract': 'In this paper, we propose mobile access to peer-reviewed medical information based on textual search and content-based visual image retrieval. Web-based interfaces designed for limited screen space were developed to query via web services a medical information retrieval engine optimizing the amount of data to be transferred in wireless form. Visual and textual retrieval engines with state-of-the-art performance were integrated. Results obtained show a good usability of the software. Future use in clinical environments has the potential of increasing quality of patient care through bedside access to the medical literature in context.', 'venue': 'IEEE Transactions on Information Technology in Biomedicine', 'year': 2012, 'authors': [{'authorId': '2566322', 'name': 'A. Depeursinge'}, {'authorId': '34416435', 'name': 'S. Duc'}, {'authorId': '1724386', 'name': 'Ivan Eggel'}, {'authorId': '2151194032', 'name': 'H. Müller'}]}, {'paperId': '9f8eb04dbafdfda997ac5e06cd6c521f82bf4e4c', 'title': 'UIMA: an architectural approach to unstructured information processing in the corporate research environment', 'abstract': \"IBM Research has over 200 people working on Unstructured Information Management (UIM) technologies with a strong focus on Natural Language Processing (NLP). These researchers are engaged in activities ranging from natural language dialog, information retrieval, topic-tracking, named-entity detection, document classification and machine translation to bioinformatics and open-domain question answering. An analysis of these activities strongly suggested that improving the organization's ability to quickly discover each other's results and rapidly combine different technologies and approaches would accelerate scientific advance. Furthermore, the ability to reuse and combine results through a common architecture and a robust software framework would accelerate the transfer of research results in NLP into IBM's product platforms. Market analyses indicating a growing need to process unstructured information, specifically multilingual, natural language text, coupled with IBM Research's investment in NLP, led to the development of middleware architecture for processing unstructured information dubbed UIMA. At the heart of UIMA are powerful search capabilities and a data-driven framework for the development, composition and distributed deployment of analysis engines. In this paper we give a general introduction to UIMA focusing on the design points of its analysis engine architecture and we discuss how UIMA is helping to accelerate research and technology transfer.\", 'venue': 'Natural Language Engineering', 'year': 2004, 'authors': [{'authorId': '2295799', 'name': 'D. Ferrucci'}, {'authorId': '144071952', 'name': 'Adam Lally'}]}, {'paperId': '14a28da0a1d5095325c5f2b97e6d17a152316d62', 'title': 'Document retrieval using knowledge-based fuzzy information retrieval techniques', 'abstract': 'A knowledge-based approach for fuzzy information retrieval is proposed, where interval queries and weighted-interval queries are allowed for document retrieval. In this paper, knowledge is represented by a concept matrix, where the elements in a concept matrix represent relevant values between concepts. The implicit relevant values between concepts are inferred by the transitive closure of the concept matrix based on fuzzy logic. The proposed method is more flexible than previous methods due to the fact that it has the capability to deal with interval queries and weighted-interval queries. >', 'venue': 'IEEE Transactions on Systems, Man and Cybernetics', 'year': 1995, 'authors': [{'authorId': '1686547', 'name': 'Shyi-Ming Chen'}, {'authorId': '2120513031', 'name': 'Jeng-Yih Wang'}]}, {'paperId': 'f87ee6e3f2c9c22493a09c5512866a640519b798', 'title': 'Towards collaboration between information seeking and information retrieval', 'abstract': \"Introduction. The conceptual framework of librarianship and information science has developed rapidly over the past decade with the prospect of application in other fields. However, transfer of concepts across branches within the field remains problematic and severely limits ability to address important information problems. Requirements. A conceptual framework is needed that will integrate the diverse areas of interest. Imperatives. An integrated framework requires sustained attention to a problem area; the application of the evolving framework to the area; the development of projects that are of relevance to more than one interest group in the field; and evolving the findings of research into the implementation of systems and services. Conclusion. The challenge facing librarianship and information science today is to bring together the allied areas of the field into an overarching conceptual framework that represents the unified whole. This paper suggests a strategy to accomplish this. Introduction: a developing framework for library and information science For many years researchers in library and information science have borrowed theory from other fields to provide insight into our research findings. We are moving from this borrowed theory approach to creating a conceptual framework that has been tested, refined and adapted specifically for application in our field. Towards collaboration between information seeking and information retrieval http://www.informationr.net/ir/10-2/paper225.html[11/12/2015 4:28:09 PM] The conceptual framework has developed rapidly during the past ten years with early signs of application in other fields. An important contribution of the research reported at the ISIC conferences is the development of the user-centred approach. This research offers understanding of information seeking and use within the various contexts of people's lives. Important meta-theories, such as Dervin's sense-making (1983) and models, such as Wilson's model of information seeking (1999) and the Information Search Process model (Kuhlthau 1991) have been developed in this research area. New ways of looking at information seeking have emerged, such as Savolainen's (1995) work on Everyday Life Information Seeking. These have substantially contributed to the conceptual framework of the field and form the basis for extensive research in user studies. However, transfer of concepts across branches within the field of library and information science remains problematic. Scholars in different areas of library and information science do not usually talk to each other, attend each other's conferences, read each other's journals or even read each other's articles in the same journal. Ellis et al. (1999)found that even within the concentration of information science, scholars do not cite across the three overlapping areas of information systems, user studies and information retrieval. This unfortunate situation severely limits our ability to solve users' information problems. There is a critical need for a broad view of library and information science incorporating concepts of each branch of the field into a unified whole. Four imperatives for building the conceptual framework The expansive field of library and information science incorporates the great traditions of librarianship, the insights of user studies, and the innovations of information retrieval and information systems. The time is right for a major initiative of collaboration across the branches of the field. I propose four imperatives for fostering collaboration and for continuing to build the conceptual framework of the field. 1. Stay with a problem long enough to verify findings and draw concepts from the findings. 2. Apply the broad conceptual framework of library and information science to inform the findings of our studies. 3. Develop research projects that incorporate concepts of interest to more than one area of the field. 4. Design application of the concepts for implementation into systems and services. Stay with a problem to develop concepts The first imperative is to stay with a problem long enough to test and verify the findings of an initial study in order to draw concepts from the findings. Sustained research is essential for developing concepts. Sustained research involves not only seeing an initial investigation through to completion but staying with a problem to verify findings and to expand understanding of that problem beyond the narrow confines of a single study. Once we are on to something we need to Towards collaboration between information seeking and information retrieval http://www.informationr.net/ir/10-2/paper225.html[11/12/2015 4:28:09 PM] follow up with further investigation applying a variety of methods to exploit the full implications of our research for the field. When we study information seeking in context it is easy to concentrate on the results specific to that context and to lose sight of underlying concepts that more generally informs physical and intellectual access to information and ideas. Here is an example from my own research on the information search process. In my first study of secondary school students I found that forming a focus in the process of information seeking was the main task rather than merely gathering information related to a topic (Kuhlthau 2004). A student who did not form a focused perspective described great difficulty writing and presenting her work. Here is how she described her dilemma. I had a general idea not a specific focus, but an idea. As I was writing, I didn't know what my focus was. When I was finished, I didn't know what my focus was. My teacher says she doesn't know what my focus was. I don't think I ever acquired a focus. It was an impossible paper to write. I would just sit there and say, 'I'm stuck'. If I learned anything from that paper it is, you have to have a focus. You have to have something to center on. You can't just have a topic. You should have an idea when you start. I had a topic but I didn't know what I wanted to do with it. I figured that when I did my research it would focus in. But I didn't let it. I kept saying, 'this is interesting and this is interesting and I'll just smush it altogether'. It didn't work out. (Kuhlthau 2004: 40). Other students talked about forming a point of view and gaining a personal perspective of the topic. From this study I drew the concept of formulation within the constructive process of information seeking. Later, in longitudinal case studies of information seeking in the workplace, I found further evidence to support the concept (Kuhlthau 2004). The securities analyst talked about finding an angle to present to his clients and the lawyers sought a strategy for presenting a case. The securities analyst explained the main problem many novice analysts had in gathering and gathering information but not being able to write the report or as he said 'get out the product'. Over and over the concept of the constructive process of forming a focus provided the insight for explaining the findings of my users' studies. Without these extensive studies the work would be interesting but not very useful for contributing to the conceptual framework of the field. Sustained, longitudinal research supports collaboration across branches of library and information science by providing confirmed findings that lead to concepts that can apply in more than one context and that more than one area of the field can use for further study and application. Apply the conceptual framework of the field The second imperative is to use the concepts developed across the field of library and information science to inform and illuminate the findings of our research Towards collaboration between information seeking and information retrieval http://www.informationr.net/ir/10-2/paper225.html[11/12/2015 4:28:09 PM] studies. Once a concept is discovered in one context it is important to study that concept in other contexts with different users. Major concepts in library and information science such as relevance, anomalous state of knowledge, and uncertainty as well as models of information seeking behavior and theoretical frameworks such as sensemaking have been examined in a variety of contexts with different types of users to ground the concept for more general application. The studies on relevance that build on Saracevic's (1975, 1996) work are an excellent example. Taylor's (1991) levels of information need and information use environments are another. These concentrations of research not only verify but also extend and develop the concepts for increased understanding and insight that make important contributions to the conceptual framework of library and information science. The concept of task complexity, developed by Bystrom and the Tampere team (Bystrom and Jarvelin 1995, Bystrom 2000, Vakkari 2001) has provided insight in my own research. I am often asked if I think that people always experience the stages of the ISP in every information seeking task. Clearly they do not. But how to differentiate between tasks was a problem for me. In my recent studies I introduced the concept of task complexity and found that workers could easily distinguish between different types of information use in complex tasks and in routine tasks. These studies revealed simple straight-forward information seeking in routine work tasks and a process of construction and formulation in those tasks identified by the user as complex (Kuhlthau 2004). One person explained that complex tasks involve a dynamic change in thinking referring to these tasks as, 'the really good ones that you lose sleep over'. These projects were found to take an extended period of time. A participant explained, 'Those are the ones that are really time consuming because you are changing your entire thinking on an industry'. And went on to explain the uncertainty in connection with complex tasks in this way, 'You feel anx\", 'venue': '', 'year': 2005, 'authors': [{'authorId': '2422188', 'name': 'C. Kuhlthau'}]}, {'paperId': '44395982079e8d33f2fb0f6ed92148ca59f8f8fd', 'title': 'Novelty and familiarity activations in PET studies of memory encoding and retrieval.', 'abstract': \"Nine young right-handed men viewed colored pictures of people, scenes, and landscapes. Then, 24 hr later while undergoing PET scanning, they viewed previously studied (OLD) pictures in one type of scan, and previously not seen (NEW) pictures in another. The OLD-NEW subtraction of PET images indicates familiarity, and the NEW-OLD indicates novelty. Familiarity activations, signalling aspects of retrieval, were observed in the left and right frontal areas, and posterior regions bilaterally. Novelty activations were in the right limbic regions, and bilaterally in temporal and parietal regions, including area 37. These latter activations were located similarly to novelty activations in previous PET studies using visual words and auditory sentences, suggesting the existence of brain regions specializing in transmodal novelty assessment. The effects of novelty are seen both behaviorally and in replicable patterns of cortical and subcortical activation. We propose a 'novelty/encoding hypothesis': (1) novelty assessment represents an early stage of long-term memory encoding; (2) elaborate, meaning-based encoding processes operate on the incoming information to the extent of its novelty, and therefore (3) the probability of long-term storage of information varies directly with the novelty of the information.\", 'venue': 'Cerebral Cortex', 'year': 1996, 'authors': [{'authorId': '3099213', 'name': 'E. Tulving'}, {'authorId': '4324425', 'name': 'H. Markowitsch'}, {'authorId': '6340994', 'name': 'F. Craik'}, {'authorId': '33793773', 'name': 'R. Habib'}, {'authorId': '1708712', 'name': 'S. Houle'}]}, {'paperId': '17c755a79dd26223b3972bb32b4a6b5775592a3c', 'title': 'A probabilistic terminological logic for modelling information retrieval', 'abstract': None, 'venue': 'Annual International ACM SIGIR Conference on Research and Development in Information Retrieval', 'year': 1994, 'authors': [{'authorId': '145077269', 'name': 'F. Sebastiani'}]}, {'paperId': '398729416a8b80594b28ae750c2c3604745d3777', 'title': 'Using latent semantic indexing for information filtering', 'abstract': 'Latent Semantic Indexing (LSI) is an information retrieval method that organizes information into a semantic structure that takes advantage of some of the implicit higher-order associations of words with text objects. The resulting structure reflects the major associative patterns in the data while ignoring some of the smaller variations that may be due to idiosyncrasies in the word usage of individual documents. This permits retrieval based on the “latent” semantic content of the documents rather than just on keyword matches. This paper evaluates using LSI for filtering information such as Netnews articles based on a model of user preferences for articles. Users judged articles on how interesting they were and based on these judgements, LSI predicted whether new articles would be judged interesting. LSI improved prediction performance over keyword matching an average of 13% and showed a 26% improvement in precision over presenting articles in the order received. The results indicate that user preferences for articles tend to cluster based on the semantic similarities between articles.', 'venue': 'Conference on Organizational Computing Systems', 'year': 1990, 'authors': [{'authorId': '2036336', 'name': 'P. Foltz'}]}, {'paperId': 'dc39505ef0c8b1bffa879a90b2b9c9de6f33f538', 'title': 'Using generative probabilistic models for multimedia retrieval', 'abstract': 'This thesis discusses information retrieval from multimedia archives, focusing on documents containing visual material. We investigate search and retrieval in collections of images and video, where video is defined as a sequence of still images. No assumptions are made with respect to the content of the documents; we concentrate on retrieval from generic, heterogeneous multimedia collections. In this research area a user\\'s query typically consists of one or more example images and the implicit request is: \"Find images similar to this one.\" In addition the query may contain a textual description of the information need. The research presented here addresses three issues within this area.', 'venue': 'SIGF', 'year': 2005, 'authors': [{'authorId': '1758740', 'name': 'T. Westerveld'}]}, {'paperId': '99f5d79da460a87b2fe802f3d70a83d58e4d0daf', 'title': 'I3R: A new approach to the design of document retrieval systems', 'abstract': 'The most effective method of improving the retrieval performance of a document retrieval system is to acquire a detailed specification of the user’s information need. The system described in this article, 13R, provides a number of facilities and search strategies based on this approach. The system uses a novel architecture to allow more than one system facility to be used at a given stage of a search session. Users influence the system actions by stating goals they wish to achieve, by evaluating system output, and by choosing particular facilities directly. The other main features of 13R are an emphasis on domain knowledge used for refining the model of the information need, and the provision of a browsing mechanism that allows the user to navigate through the knowledge base.', 'venue': 'Journal of the American Society for Information Science', 'year': 1987, 'authors': [{'authorId': '144456145', 'name': 'W. Bruce Croft'}, {'authorId': '2107283746', 'name': 'R. Thompson'}]}, {'paperId': 'bd49e7ecf477766790840eabbefcbbcb3b998d72', 'title': 'Database Resources of the National Center for Biotechnology Information', 'abstract': 'The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed database of citations and abstracts for published life science journals. The Entrez system provides search and retrieval operations for most of these data from 37 distinct databases. The E-utilities serve as the programming interface for the Entrez system. Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. New resources released in the past year include iCn3D, MutaBind, and the Antimicrobial Resistance Gene Reference Database; and resources that were updated in the past year include My Bibliography, SciENcv, the Pathogen Detection Project, Assembly, Genome, the Genome Data Viewer, BLAST and PubChem. All of these resources can be accessed through the NCBI home page at www.ncbi.nlm.nih.gov.', 'venue': 'Nucleic Acids Res.', 'year': 2016, 'authors': [{'authorId': '51341623', 'name': 'D. Vakatov'}, {'authorId': '2188116', 'name': 'E. Yaschenko'}]}, {'paperId': '161a8e11dd11a4f47c1cb66d4015d1b4afbab0fd', 'title': 'A finger on the pulse: temporal rhythms and information seeking in medical work', 'abstract': 'Most cooperative work takes place in information-rich environments. However, studies of \"information work\" tend to focus on the decontextualized access and retrieval problems faced by individual information seekers. Our work is directed towards understanding how information management is seamlessly integrated into the course of everyday activities. Drawing on an ethnographic study of medical work, we explore the relationship between information and temporal coordination and discuss the role of temporal patterns or \"rhythms\" in providing individuals with the means to coordinate information and work.', 'venue': 'Conference on Computer Supported Cooperative Work', 'year': 2002, 'authors': [{'authorId': '145526578', 'name': 'Madhu C. Reddy'}, {'authorId': '1762952', 'name': 'P. Dourish'}]}, {'paperId': 'a6b33dafcd6a55e8d51d95c7fd62ae6b76fc6e4e', 'title': 'Beyond TFIDF Weighting for Text Categorization in the Vector Space Model', 'abstract': 'KNN and SVM are two machine learning approaches to Text Categorization (TC) based on the Vector Space Model. In this model, borrowed from Information Retrieval, documents are represented as a vector where each component is associated with a particular word from the vocabulary. Traditionally, each component value is assigned using the information retrieval TFIDF measure. While this weighting method seems very appropriate for IR, it is not clear that it is the best choice for TC problems. Actually, this weighting method does not leverage the information implicitly contained in the categorization task to represent documents. In this paper, we introduce a new weighting method based on statistical estimation of the importance of a word for a specific categorization problem. This method also has the benefit to make feature selection implicit, since useless features for the categorization problem considered get a very small weight. Extensive experiments reported in the paper shows that this new weighting method improves significantly the classification accuracy as measured on many categorization tasks.', 'venue': 'International Joint Conference on Artificial Intelligence', 'year': 2005, 'authors': [{'authorId': '36530411', 'name': 'P. Soucy'}, {'authorId': '2237381', 'name': 'G. Mineau'}]}, {'paperId': 'd7683ab8419e2b5dca212ca19018fb457bf1a488', 'title': 'The Intellectual Foundation of Information Organization', 'abstract': 'Instant electronic access to digital information is the single most distinguishing attribute of the information age. The elaborate retrieval mechanisms that support such access are a product of technology. But technology is not enough. The effectiveness of a system for accessing information is a direct function of the intelligence put into organizing it. Just as the practical field of engineering has theoretical physics as its underlying base, the design of systems for organizing information rests on an intellectual foundation. The subject of this book is the systematized body of knowledge that constitutes this foundation.Integrating the disparate disciplines of descriptive cataloging, subject cataloging, indexing, and classification, the book adopts a conceptual framework that views the process of organizing information as the use of a special language of description called a bibliographic language. The book is divided into two parts. The first part is an analytic discussion of the intellectual foundation of information organization. The second part moves from generalities to particulars, presenting an overview of three bibliographic languages: work languages, document languages, and subject languages. It looks at these languages in terms of their vocabulary, semantics, and syntax. The book is written in an exceptionally clear style, at a level that makes it understandable to those outside the discipline of library and information science.Digital Libraries and Electronic Publishing series', 'venue': '', 'year': 2000, 'authors': [{'authorId': '3302562', 'name': 'Elaine Svenonius'}]}]}\n",
            "File is in the destination.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJtRnhLxP0Bh"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data. \n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "\n",
        "# Authenticate to Twitter\n",
        "auth = tweepy.OAuthHandler(\"consumer_key\", \"consumer_secret\")\n",
        "auth.set_access_token(\"access_token\", \"access_token_secret\")\n",
        "\n",
        "# Create API object\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "# Define search terms and number of posts to collect\n",
        "search_terms = \"#examplehashtag\"\n",
        "number_of_posts = 1000\n",
        "\n",
        "# Collect tweets\n",
        "posts = tweepy.Cursor(api.search_tweets,\n",
        "              q=search_terms,\n",
        "              tweet_mode='extended').items(number_of_posts)\n",
        "\n",
        "# Store information in a list\n",
        "collected_posts = []\n",
        "for post in posts:\n",
        "    collected_posts.append({\n",
        "        \"User_name\": post.user.screen_name,\n",
        "        \"Posted_time\": post.created_at,\n",
        "        \"Text\": post.full_text\n",
        "    })\n",
        "\n",
        "# Output collected information\n",
        "for post in collected_posts:\n",
        "    print(post[\"User_name\"], post[\"Posted_time\"], post[\"Text\"])\n"
      ],
      "metadata": {
        "id": "b6CfWD05xnzJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}